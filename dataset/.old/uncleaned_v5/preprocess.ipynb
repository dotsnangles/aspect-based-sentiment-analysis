{"cells":[{"cell_type":"code","execution_count":142,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1666249058207,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"MbTADoPibruA"},"outputs":[],"source":["from transformers import (\n","    AutoTokenizer, AutoModelForMaskedLM, \n",")\n","\n","# import nlpaug.augmenter.char as nac\n","# import nlpaug.augmenter.word as naw\n","# import nlpaug.augmenter.sentence as nas\n","# import nlpaug.flow as nafc\n","# from nlpaug.util import Action\n","\n","# from googletrans import Translator\n","# import translators as ts\n","\n","import re, math, random, json\n","from copy import deepcopy\n","from sklearn.model_selection import StratifiedKFold\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import Counter\n","\n","from module.preprocess import decorate_form, decorate_acd_pair, decorate_asc_pair, decorate_acd_pair_split, decorate_asc_pair_split\n","\n","import demoji\n","\n","# from cleantext import clean\n","# from pykospacing import Spacing\n","# from hanspell import spell_checker"]},{"cell_type":"markdown","metadata":{},"source":["# Load Raw Data"]},{"cell_type":"code","execution_count":143,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666249059795,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"NjTs0B-pbxm6"},"outputs":[],"source":["train_json = './dataset/nikluge-sa-2022-train.jsonl'\n","dev_json = './dataset/nikluge-sa-2022-dev.jsonl'\n","test_json = './dataset/nikluge-sa-2022-test.jsonl'\n","\n","train = pd.read_json(train_json, lines=True)\n","dev = pd.read_json(dev_json, lines=True)\n","test = pd.read_json(test_json, lines=True)\n","\n","train = train.drop(2319)\n","dev = dev.drop(1692)"]},{"cell_type":"markdown","metadata":{},"source":["# Declare Stuff to use"]},{"cell_type":"code","execution_count":144,"metadata":{"id":"ov3SPo0UK_2k"},"outputs":[],"source":["### new\n","entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","\n","]\n","more_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","\n","tf_id_to_name = ['True', 'False']\n","tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n","\n","polarity_id_to_name = ['positive', 'negative', 'neutral']\n","polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}"]},{"cell_type":"markdown","metadata":{},"source":["# Count"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tags found:  3254\n","tag set of df:  22\n","tag set of offered:  25\n","difference:  {'패키지/구성품#가격', '제품 전체#다양성', '브랜드#디자인'}\n"]},{"data":{"text/plain":["[('본품#품질', 1196),\n"," ('제품 전체#일반', 847),\n"," ('본품#일반', 256),\n"," ('제품 전체#품질', 242),\n"," ('제품 전체#디자인', 158),\n"," ('본품#편의성', 95),\n"," ('제품 전체#편의성', 94),\n"," ('제품 전체#인지도', 66),\n"," ('패키지/구성품#디자인', 54),\n"," ('브랜드#일반', 53),\n"," ('제품 전체#가격', 49),\n"," ('패키지/구성품#편의성', 38),\n"," ('패키지/구성품#일반', 30),\n"," ('본품#다양성', 21),\n"," ('본품#디자인', 15),\n"," ('브랜드#품질', 13),\n"," ('패키지/구성품#품질', 11),\n"," ('브랜드#인지도', 9),\n"," ('브랜드#가격', 3),\n"," ('본품#가격', 2),\n"," ('패키지/구성품#다양성', 1),\n"," ('본품#인지도', 1)]"]},"execution_count":145,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.concat([train, dev])\n","df = pd.concat([train])\n","\n","count = 0\n","tags = []\n","ner_inputs = []\n","for idx, row in df.iterrows():\n","    if len(row.annotation) > 0:\n","        for annotation in row.annotation:\n","            form = row.sentence_form\n","            if annotation[1][0] != None and annotation[1][2] > 0:\n","                tags.append(annotation[0])\n","                ner_input = []\n","                start = annotation[1][1]\n","                end = annotation[1][2]\n","                if start != 0:\n","                    ner_input.append(form[:start])\n","                ner_input.append(form[start:end])\n","                if len(form) != end:\n","                    ner_input.append(form[end:])\n","                count += 1\n","            else:\n","                tags.append(annotation[0])\n","                ner_input = []\n","                ner_input.append(form)\n","                count += 1\n","            ner_inputs.append(ner_input)\n","\n","print('tags found: ', count)\n","print('tag set of df: ', len(set(tags)))\n","print('tag set of offered: ', len(set(entity_property_pair)))\n","print('difference: ', set(entity_property_pair)-set(tags))\n","\n","tag_counter = Counter(tags)\n","sorted(tag_counter.items(), key=lambda x: x[1], reverse=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Filter entity_property_pair and Drop rows accordingly"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[],"source":["FILTER_MODE = False\n","\n","filter = ['본품#품질',\n","          '제품 전체#일반',\n","          '본품#일반',\n","          '제품 전체#품질',\n","          '제품 전체#디자인',\n","          '본품#편의성',\n","          '제품 전체#편의성',\n","          '제품 전체#인지도',\n","          '패키지/구성품#디자인',\n","          '브랜드#일반',\n","          '제품 전체#가격']\n","\n","\n","def remove_props(df):\n","    for idx, row in df.iterrows():\n","        empty = []\n","        stay = True\n","        for annotation in row.annotation:\n","            if annotation[0] not in filter or annotation[2] != 'positive':\n","                stay = False\n","        if stay == False:\n","            row.annotation = empty\n","    df['check'] = df.annotation.apply(lambda x: bool(x))\n","    df = df.drop(df[df.check == False].index)\n","    return df"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[{"data":{"text/plain":["(3000, 2793)"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["if FILTER_MODE == True:\n","    train = remove_props(train)\n","    dev = remove_props(dev)\n","len(train), len(dev)"]},{"cell_type":"markdown","metadata":{"id":"hkFQP8GW4SXT"},"source":["# Preprocess"]},{"cell_type":"markdown","metadata":{},"source":["## Cleansing"]},{"cell_type":"markdown","metadata":{"id":"PII4v9rJLzaV"},"source":["### Before"]},{"cell_type":"code","execution_count":148,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666249062532,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"oBhyEPcfmFdB","outputId":"362f9678-bda2-4ded-f372-23c0e9fdd36d"},"outputs":[],"source":["# for el in train.sample(n=5).sentence_form:\n","#     print(el)"]},{"cell_type":"code","execution_count":149,"metadata":{"executionInfo":{"elapsed":5570,"status":"ok","timestamp":1666249070279,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"zxaftwE_uq9w"},"outputs":[],"source":["# train.sentence_form = train.sentence_form.apply(preprocess)\n","# dev.sentence_form = dev.sentence_form.apply(preprocess)\n","# test.sentence_form = test.sentence_form.apply(preprocess)\n","# total = pd.concat([train, dev])"]},{"cell_type":"markdown","metadata":{"id":"B28qDpz-L47B"},"source":["### Test"]},{"cell_type":"code","execution_count":150,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1666249157277,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"TmQV4tDyycbX"},"outputs":[],"source":["# case = total.sentence_form.str.contains('r[^A-Za-z0-9가-힣\\s]+', case=False, flags=0, na=None, regex=True)\n","# for e in total[case].sentence_form:\n","#     print(e)"]},{"cell_type":"markdown","metadata":{"id":"ymHR_z-7L3LW"},"source":["### After"]},{"cell_type":"code","execution_count":151,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666249070568,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"I94k-mG7s_q5","outputId":"9e82bf97-1530-4783-9fb3-80c82299a3c3"},"outputs":[],"source":["# for i, row in total[['id', 'sentence_form']].sample(n=5).iterrows():\n","#     print(row.id, '\\t', row.sentence_form)"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["# total['check'] = total.sentence_form.str.find('OO')\n","# for row in total[total.check > -1].sentence_form:\n","#     print(row)\n","#     break"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["# total"]},{"cell_type":"markdown","metadata":{"id":"25AnAYGke6BQ"},"source":["## Reformat"]},{"cell_type":"code","execution_count":154,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666184223688,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"cRZR6lTRED9s","outputId":"08f43287-453a-4377-fe1b-e7d8b3fe6fd8"},"outputs":[{"data":{"text/plain":["25"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["len(entity_property_pair)"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[{"data":{"text/plain":["(<function module.preprocess.decorate_form(form)>,\n"," <function module.preprocess.decorate_acd_pair(entity)>,\n"," <function module.preprocess.decorate_asc_pair(entity, sentiment)>,\n"," <function module.preprocess.decorate_acd_pair_split(entity)>,\n"," <function module.preprocess.decorate_asc_pair_split(entity, sentiment)>)"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["decorate_form, decorate_acd_pair, decorate_asc_pair, decorate_acd_pair_split, decorate_asc_pair_split"]},{"cell_type":"code","execution_count":156,"metadata":{"id":"9K9UG5aybovX"},"outputs":[],"source":["def reformat(df):\n","    ep =[]\n","    p = []\n","    for index, row in df.iterrows():\n","        utterance = row.sentence_form\n","        id = row.id\n","        \n","        # form = utterance\n","        form = decorate_form(utterance)\n","\n","        for pair in entity_property_pair:\n","            isPairInOpinion = False\n","            if pd.isna(utterance):\n","                break\n","            for annotation in row.annotation:\n","                entity_property = annotation[0]\n","                sentiment = annotation[2]\n","                if entity_property == pair:\n","                    \n","                    # acd_pair = entity_property\n","                    # acd_pair = decorate_acd_pair(entity_property)\n","                    acd_pair = decorate_acd_pair_split(entity_property)\n","                    \n","                    ep_append = [id, form, acd_pair, tf_name_to_id['True']]\n","                    ep.append(ep_append)\n","                    p.append([id, utterance, entity_property, sentiment])\n","                    isPairInOpinion = True\n","                    break\n","            if isPairInOpinion is False:\n","                \n","                # acd_pair = pair\n","                # acd_pair = decorate_acd_pair(pair)\n","                acd_pair = decorate_acd_pair_split(pair)\n","                \n","                ep_append = [id, form, acd_pair, tf_name_to_id['False']]\n","                ep.append(ep_append)\n","    return ep, p"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":["def reformat_p_binary(df):\n","    p_binary = []\n","    for i, row in df.iterrows():\n","        row.id, row.form, row.pair, row.sentiment\n","        \n","        # form = row.form\n","        form = decorate_form(row.form)\n","        \n","        for sentiment in polarity_id_to_name:\n","            if sentiment == row.sentiment:\n","\n","                # asc_pair = '#'.join([row.pair, row.sentiment])\n","                # asc_pair = decorate_asc_pair(row.pair, row.sentiment)\n","                asc_pair = decorate_asc_pair_split(row.pair, row.sentiment)\n","\n","                p_binary_append = [row.id, form, asc_pair, tf_name_to_id['True']]\n","                p_binary.append(p_binary_append)\n","            else:\n","\n","                # asc_pair = '#'.join([row.pair, sentiment])\n","                # asc_pair = decorate_asc_pair(row.pair, sentiment)\n","                asc_pair = decorate_asc_pair_split(row.pair, sentiment)\n","\n","                p_binary_append = [row.id, form, asc_pair, tf_name_to_id['False']]\n","                p_binary.append(p_binary_append)\n","    return p_binary"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"data":{"text/plain":["(3000, 2793)"]},"execution_count":158,"metadata":{},"output_type":"execute_result"}],"source":["len(train), len(dev)"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":["entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","\n","]"]},{"cell_type":"code","execution_count":160,"metadata":{"id":"unJ9pUXwYTTm"},"outputs":[{"data":{"text/plain":["(75000, 69825, 3196, 3002)"]},"execution_count":160,"metadata":{},"output_type":"execute_result"}],"source":["ep_train, p_train = reformat(train)\n","ep_dev, p_dev = reformat(dev)\n","\n","ep_train = pd.DataFrame(ep_train, columns=['id', 'form', 'pair', 'labels'])\n","ep_dev = pd.DataFrame(ep_dev, columns=['id', 'form', 'pair', 'labels'])\n","\n","p_train = pd.DataFrame(p_train, columns=['id', 'form', 'pair', 'sentiment'])\n","p_dev = pd.DataFrame(p_dev, columns=['id', 'form', 'pair', 'sentiment'])\n","\n","len(ep_train), len(ep_dev), len(p_train), len(p_dev)"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"data":{"text/plain":["(75000, 69825, 9588, 9006)"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["p_binary_train = reformat_p_binary(p_train)\n","p_binary_train = pd.DataFrame(p_binary_train, columns=['id', 'form', 'pair', 'labels'])\n","\n","p_binary_dev = reformat_p_binary(p_dev)\n","p_binary_dev = pd.DataFrame(p_binary_dev, columns=['id', 'form', 'pair', 'labels'])\n","\n","len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev)"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[],"source":["# ep_train.sort_values(['id', 'labels'], inplace=True)\n","# ep_dev.sort_values(['id', 'labels'], inplace=True)\n","# p_binary_train.sort_values(['id', 'labels'], inplace=True, ascending=[True, True])\n","# p_binary_dev.sort_values(['id', 'labels'], inplace=True, ascending=[True, True])"]},{"cell_type":"markdown","metadata":{"id":"rjaoyM97e9PQ"},"source":["### Counting"]},{"cell_type":"code","execution_count":141,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"GFroeNPQc-DZ","outputId":"bce5e0c2-0423-4db7-982a-23d5b68b8618"},"outputs":[{"name":"stdout","output_type":"stream","text":["binary_multi: 75000 69825 3196 3002\n","binary_binary: 75000 69825 9588 9006\n","\n","after drop_duplicates\n","\n","binary_multi: 75000 69825 3196 3002\n","binary_binary: 75000 69825 9588 9006\n"]},{"data":{"text/plain":["(None, None)"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["print('binary_multi: ', end=''), print(len(ep_train), len(ep_dev), len(p_train), len(p_dev))\n","print('binary_binary: ', end=''), print(len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev))\n","ep_train = ep_train.drop_duplicates()\n","ep_dev = ep_dev.drop_duplicates()\n","p_train = p_train.drop_duplicates()\n","p_dev = p_dev.drop_duplicates()\n","p_binary_train = p_binary_train.drop_duplicates()\n","p_binary_dev = p_binary_dev.drop_duplicates()\n","print('\\nafter drop_duplicates\\n')\n","print('binary_multi: ', end=''), print(len(ep_train), len(ep_dev), len(p_train), len(p_dev))\n","print('binary_binary: ', end=''), print(len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev))"]},{"cell_type":"markdown","metadata":{},"source":["### Validate Here"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["nikluge-sa-2022-train-00001 \n"," 상품평 문장: <<둘쨋날은 미친듯이 밟아봤더니 기어가 헛돌면서 틱틱 소리가 나서 경악.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00001 \n"," 상품평 문장: <<둘쨋날은 미친듯이 밟아봤더니 기어가 헛돌면서 틱틱 소리가 나서 경악.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00001 \n"," 상품평 문장: <<둘쨋날은 미친듯이 밟아봤더니 기어가 헛돌면서 틱틱 소리가 나서 경악.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00002 \n"," 상품평 문장: <<이거 뭐 삐꾸를 준 거 아냐 불안하고, 거금 투자한 게 왜 이래.. 싶어서 정이 확 떨어졌는데 산 곳 가져가서 확인하니 기어 텐션 문제라고 고장 아니래.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00002 \n"," 상품평 문장: <<이거 뭐 삐꾸를 준 거 아냐 불안하고, 거금 투자한 게 왜 이래.. 싶어서 정이 확 떨어졌는데 산 곳 가져가서 확인하니 기어 텐션 문제라고 고장 아니래.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00002 \n"," 상품평 문장: <<이거 뭐 삐꾸를 준 거 아냐 불안하고, 거금 투자한 게 왜 이래.. 싶어서 정이 확 떨어졌는데 산 곳 가져가서 확인하니 기어 텐션 문제라고 고장 아니래.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00003 \n"," 상품평 문장: <<간사하게도 그 이후에는 라이딩이 아주 즐거워져서 만족스럽게 탔다.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00003 \n"," 상품평 문장: <<간사하게도 그 이후에는 라이딩이 아주 즐거워져서 만족스럽게 탔다.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00003 \n"," 상품평 문장: <<간사하게도 그 이후에는 라이딩이 아주 즐거워져서 만족스럽게 탔다.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00004 \n"," 상품평 문장: <<샥이 없는 모델이라 일반 도로에서 타면 노면의 진동 때문에 손목이 덜덜덜 떨리고 이가 부딪칠 지경인데 이마저도 며칠 타면서 익숙해지니 신경쓰이지 않게 됐다.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00004 \n"," 상품평 문장: <<샥이 없는 모델이라 일반 도로에서 타면 노면의 진동 때문에 손목이 덜덜덜 떨리고 이가 부딪칠 지경인데 이마저도 며칠 타면서 익숙해지니 신경쓰이지 않게 됐다.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00004 \n"," 상품평 문장: <<샥이 없는 모델이라 일반 도로에서 타면 노면의 진동 때문에 손목이 덜덜덜 떨리고 이가 부딪칠 지경인데 이마저도 며칠 타면서 익숙해지니 신경쓰이지 않게 됐다.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00005 \n"," 상품평 문장: <<안장도 딱딱해서 엉덩이가 아팠는데 무시하고 타고 있다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00005 \n"," 상품평 문장: <<안장도 딱딱해서 엉덩이가 아팠는데 무시하고 타고 있다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00005 \n"," 상품평 문장: <<안장도 딱딱해서 엉덩이가 아팠는데 무시하고 타고 있다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00006 \n"," 상품평 문장: <<지금 내 실력과 저질 체력으로는 이 정도 자전거도 되게 훌륭한 거라는..>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00006 \n"," 상품평 문장: <<지금 내 실력과 저질 체력으로는 이 정도 자전거도 되게 훌륭한 거라는..>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00006 \n"," 상품평 문장: <<지금 내 실력과 저질 체력으로는 이 정도 자전거도 되게 훌륭한 거라는..>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00007 \n"," 상품평 문장: <<내장 기어 3단은 썩 좋은 물건이라 기어 변환도 부드럽고 겉에서는 기어가 보이지 않기 때문에 깔끔하다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00007 \n"," 상품평 문장: <<내장 기어 3단은 썩 좋은 물건이라 기어 변환도 부드럽고 겉에서는 기어가 보이지 않기 때문에 깔끔하다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00007 \n"," 상품평 문장: <<내장 기어 3단은 썩 좋은 물건이라 기어 변환도 부드럽고 겉에서는 기어가 보이지 않기 때문에 깔끔하다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00008 \n"," 상품평 문장: <<한번 교환했는데 새로 온 UD20은 불량화소가 있고 ㅜ ㅜ ㅜ>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00008 \n"," 상품평 문장: <<한번 교환했는데 새로 온 UD20은 불량화소가 있고 ㅜ ㅜ ㅜ>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00008 \n"," 상품평 문장: <<한번 교환했는데 새로 온 UD20은 불량화소가 있고 ㅜ ㅜ ㅜ>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00009 \n"," 상품평 문장: <<전에 작동 안되었던 자막 검색 후 등록 기능이 똑같이 작동 안 된다!!!>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00009 \n"," 상품평 문장: <<전에 작동 안되었던 자막 검색 후 등록 기능이 똑같이 작동 안 된다!!!>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00009 \n"," 상품평 문장: <<전에 작동 안되었던 자막 검색 후 등록 기능이 똑같이 작동 안 된다!!!>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00010 \n"," 상품평 문장: <<왜 [등록]키를 만들어놓고 제대로 단어장에 등록이 되지 않는 거냐!!>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00010 \n"," 상품평 문장: <<왜 [등록]키를 만들어놓고 제대로 단어장에 등록이 되지 않는 거냐!!>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00010 \n"," 상품평 문장: <<왜 [등록]키를 만들어놓고 제대로 단어장에 등록이 되지 않는 거냐!!>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00011 \n"," 상품평 문장: <<다른 부가 기능은 참 훌륭한데..>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00011 \n"," 상품평 문장: <<다른 부가 기능은 참 훌륭한데..>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00011 \n"," 상품평 문장: <<다른 부가 기능은 참 훌륭한데..>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00012 \n"," 상품평 문장: <<미치겠네.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00012 \n"," 상품평 문장: <<미치겠네.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00012 \n"," 상품평 문장: <<미치겠네.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00013 \n"," 상품평 문장: <<아.. 진짜 기계 사겠나.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00013 \n"," 상품평 문장: <<아.. 진짜 기계 사겠나.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00013 \n"," 상품평 문장: <<아.. 진짜 기계 사겠나.>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00014 \n"," 상품평 문장: <<이번에는 사전까지..>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00014 \n"," 상품평 문장: <<이번에는 사전까지..>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00014 \n"," 상품평 문장: <<이번에는 사전까지..>> \n"," 상품평 문장의 대범주 유형이 <<제품 전체>>이고 소범주 유형은 <<일반>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00015 \n"," 상품평 문장: <<동영상 재생하면서 자막 중 모르는 내용 있으면 터치해서 바로 검색하는 기능 때문에 산 건데 이게 에러다..>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00015 \n"," 상품평 문장: <<동영상 재생하면서 자막 중 모르는 내용 있으면 터치해서 바로 검색하는 기능 때문에 산 건데 이게 에러다..>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00015 \n"," 상품평 문장: <<동영상 재생하면서 자막 중 모르는 내용 있으면 터치해서 바로 검색하는 기능 때문에 산 건데 이게 에러다..>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00016 \n"," 상품평 문장: <<어제 샀는데 오늘 안 되더군.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00016 \n"," 상품평 문장: <<어제 샀는데 오늘 안 되더군.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n","nikluge-sa-2022-train-00016 \n"," 상품평 문장: <<어제 샀는데 오늘 안 되더군.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<neutral>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00017 \n"," 상품평 문장: <<아까 한 번 잠깐 되더니 지금 또 등록 버튼이 먹통이다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<positive>>이다. \n"," 1 \n","\n","nikluge-sa-2022-train-00017 \n"," 상품평 문장: <<아까 한 번 잠깐 되더니 지금 또 등록 버튼이 먹통이다.>> \n"," 상품평 문장의 대범주 유형이 <<본품>>이고 소범주 유형은 <<품질>>일 때 감성 유형은 <<negative>>이다. \n"," 0 \n","\n"]}],"source":["df = p_binary_train\n","for idx, row in df.iterrows():\n","    print(row.id, '\\n',\n","          row.form, '\\n',\n","          row.pair, '\\n',\n","          row.labels,  '\\n',)\n","    if idx == 49:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["### Save"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["./dataset/uncleaned_v5\n"]}],"source":["DATA_V = 'uncleaned_v5'\n","save_path = f'./dataset/{DATA_V}'\n","print(save_path)"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!mkdir -p {save_path}\n","\n","train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","\n","p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Additional Length Test If Needed"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[],"source":["# ep_train, ep_dev, p_binary_train, p_binary_dev"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["30000\n","\n","\n","\n","3060\n","30117\n"]}],"source":["model_checkpoint = 'snunlp/KR-ELECTRA-discriminator'\n","\n","train_path = f'./dataset/{DATA_V}/raw_train.csv'\n","dev_path = f'./dataset/{DATA_V}/raw_dev.csv'\n","test_path = f'./dataset/{DATA_V}/raw_test.csv'\n","train = pd.read_csv(train_path)\n","dev = pd.read_csv(dev_path)\n","test = pd.read_csv(test_path)\n","\n","### new\n","entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","]\n","special_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","emojis = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))\n","ep_labels = pd.Series(entity_property_pair, name='sentence_form', copy=True)\n","\n","tokens2add = special_tokens + emojis\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","print(len(tokenizer))\n","tokenizer_train_data = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame().drop_duplicates()\n","tokenizer_train_data = tokenizer_train_data.sentence_form.to_list()\n","new_tokenizer = tokenizer.train_new_from_iterator(tokenizer_train_data, vocab_size=1)\n","new_tokens = set(list(new_tokenizer.vocab.keys()) + tokens2add) - set(tokenizer.vocab.keys())\n","tokenizer.add_tokens(list(new_tokens))\n","print(len(new_tokenizer))\n","print(len(tokenizer))\n","# model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["ep_train, ep_dev, p_binary_train, p_binary_dev\n","len_counter = []\n","for df in [ep_train, ep_dev, p_binary_train, p_binary_dev]:\n","    for idx, row in df.iterrows():\n","        len_counter.append(len(tokenizer(row[\"form\"], row[\"pair\"], truncation=True).input_ids))"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"data":{"text/plain":["150"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["max(len_counter)"]},{"cell_type":"markdown","metadata":{},"source":["### done here."]},{"cell_type":"markdown","metadata":{},"source":["## Save Files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save_path = './dataset/cleaned_v1'\n","\n","# train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","# dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","# test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","# ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","# ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","# p_train.to_csv(f'{save_path}/pc_train.csv', index=False)\n","# p_dev.to_csv(f'{save_path}/pc_dev.csv', index=False)\n","# p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","# p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"jAr5_Zc6HHQV"},"source":["# ASC Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJxucvEoipu2"},"outputs":[],"source":["model_checkpoint = '/content/drive/MyDrive/aspect_based_sentiment_analysis/base_model/klue_roberta_base/v2/klue_roberta_base_mlm/checkpoint-19860'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOg2ROQQi2OQ"},"outputs":[],"source":["sTokens = tokenizer.all_special_tokens\n","\n","def delTokens(sent):\n","    sent = sent.split(' ')\n","    temp = []\n","    for e in sent:\n","        if e not in sTokens:\n","            temp.append(e)\n","    return ' '.join(temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abyMklzeI81o"},"outputs":[],"source":["positive, negative, neutral = p_train[p_train.sentiment == 'positive'], p_train[p_train.sentiment == 'negative'], p_train[p_train.sentiment == 'neutral']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666184227323,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Vl33p6WhHmOj","outputId":"ff54ba6a-ccd2-4fb2-f716-03a4469cc43d"},"outputs":[],"source":["len(positive), len(negative), len(neutral)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666184227323,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Tt1lAdOUHDyq","outputId":"8c17fd2d-5a8b-4f7a-ac7b-8a69604a8281"},"outputs":[],"source":["(58 * 3) * 4 * 3, (95 * 3) * 4 * 2 # bt ri rr"]},{"cell_type":"markdown","metadata":{"id":"nI6TIyYLINET"},"source":["Back Translation / Random Insertion / Random Replacement / Random Swap / Random Deletion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeypyouVr6Vk"},"outputs":[],"source":["def backTrans(text):\n","    aug1 = ts.papago(text, sleep_seconds=5, from_language='ko', to_language='en')\n","    aug1 = ts.papago(aug1, sleep_seconds=5, from_language='en', to_language='ko')\n","\n","    aug2 = ts.papago(text, sleep_seconds=5, from_language='ko', to_language='ja')\n","    aug2 = ts.papago(aug2, sleep_seconds=5, from_language='ja', to_language='ko')\n","\n","    return [aug1, aug2]\n","\n","def randomInsert(num, sample, device):\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path=model_checkpoint, action=\"insert\", model_type='bert', top_k=5, aug_p=0.3, aug_min=1, aug_max=1, device=device)\n","\n","    aug_result = aug.augment(sample, n=num, num_thread=12)\n","    aug_result = list(map(delTokens, aug_result))\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomReplace(num, sample, device):\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path=model_checkpoint, action=\"insert\", model_type='bert', top_k=5, aug_p=0.3, aug_min=1, aug_max=1, device=device)\n","\n","    aug_result = aug.augment(sample, n=num, num_thread=12)\n","    aug_result = list(map(delTokens, aug_result))\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomSwap(num, sample):\n","    aug = naw.RandomWordAug(action='swap', aug_min=1, aug_max=1, aug_p=0.3)    \n","    aug_result = aug.augment(sample, n=num, num_thread=2)\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomSplit(num, sample):\n","    aug = naw.SplitAug(aug_min=1, aug_max=1, aug_p=0.3, min_char=3)\n","    aug_result = aug.augment(sample, n=num, num_thread=2)\n","    aug_result = list(set(aug_result))\n","    return aug_result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666184227324,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"S61gxehDqAsf","outputId":"a5a79918-c784-41e5-a0f2-7f0006086908"},"outputs":[],"source":["(58 * 3) * 5 * 4, (95 * 3) * 4 * 3 # bt ri rr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QVPawzK3Uqt"},"outputs":[],"source":["def backtransRoutine(data2augment, output_path):\n","    print('back translation started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = backTrans(row[1])\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(len(f'back translation finished.\\ncurrent count: {len(data2augment)}'))\n","\n","    data_aug = pd.DataFrame(data2augment, columns=['id', 'sentence_form', 'entity_property', 'sentiment'])\n","    data_aug.to_csv(f'{output_path}', index=False)\n","\n","    return data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ab6cjfGYqpJG"},"outputs":[],"source":["import os\n","\n","def edaRoutine(data2augment, ri, rr, output_path):\n","    print(f'current count: {len(data2augment)}')\n","    print('random insertion started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = randomInsert(ri, row[1], 'cuda')\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(f'random insertion finished.\\ncurrent count: {len(data2augment)}')\n","\n","    print('random replacement started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = randomReplace(rr, row[1], 'cuda')\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(f'random replacement finished.\\ncurrent count: {len(data2augment)}')\n","\n","    print('random swap and split started.')\n","    while len(data2augment) < len(positive):\n","        temp = []\n","        k = random.randrange(len(negative))\n","        id, text, entity, sentiment = data2augment[k]\n","\n","        selector = random.randint(0,1)\n","        if selector == 0:\n","            augs = randomSwap(1, text)\n","            for aug in augs:\n","                if aug != '' and aug != text:\n","                    new = [id, aug, entity, sentiment]\n","                    if new not in data2augment:\n","                        temp.append(new)\n","            data2augment.extend(temp)\n","        else:\n","            augs = randomSplit(1, text)\n","            for aug in augs:\n","                if aug != '' and aug != text:\n","                    new = [id, aug, entity, sentiment]\n","                    if new not in data2augment:\n","                        temp.append(new)\n","            data2augment.extend(temp)\n","        if len(data2augment)%25 == 0:\n","            print(f'random swap and split in progress.\\ncurrent count: {len(data2augment)}')\n","\n","    print(f'whole augmentation routine finished.\\ntotal count: {len(data2augment)}')\n","\n","    data_aug = pd.DataFrame(data2augment, columns=['id', 'sentence_form', 'entity_property', 'sentiment'])\n","    data_aug.to_csv(f'{output_path}', index=False)\n","\n","    return data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmx-J4bu1JuU"},"outputs":[],"source":["### negative\n","# # back translation\n","\n","# data2augment = negative.values.tolist()\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'negative_bt.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# negative_bt = backtransRoutine(data2augment, output_path)\n","negative_bt = pd.read_csv(output_path)\n","negative_bt = negative_bt.values.tolist()\n","# RI / RR\n","\n","ri = 4 # times - 1\n","rr = 3 # times - 1\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'negative_aug.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# negative_aug = edaRoutine(negative_bt, ri, rr, output_path)\n","negative_aug = pd.read_csv(output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qu5fuqt5Q6AY"},"outputs":[],"source":["negative_aug\n","negative_aug = negative_aug.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GUBfkU5QIZD"},"outputs":[],"source":["# negative_aug.sample(n=15, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eP_iTJrASqcr"},"outputs":[],"source":["# negative_aug.sort_values('id').head(50).sentence_form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XROBqEb71MmR"},"outputs":[],"source":["### neutral\n","# back translation\n","\n","# data2augment = neutral.values.tolist()\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'neutral_bt.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# neutral_bt = backtransRoutine(data2augment, output_path)\n","neutral_bt = pd.read_csv(output_path)\n","neutral_bt = neutral_bt.values.tolist()\n","\n","# RI / RR\n","\n","ri = 3 # times - 1\n","rr = 2 # times - 1\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'neutral_aug.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# neutral_aug = edaRoutine(neutral_bt, ri, rr, output_path)\n","neutral_aug = pd.read_csv(output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MS1g9SKERC1T"},"outputs":[],"source":["neutral_aug\n","neutral_aug = neutral_aug.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC38BqB7RFMr"},"outputs":[],"source":["# neutral_aug.sample(n=15, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v5pqYIlRheZ"},"outputs":[],"source":["# neutral_aug.sort_values('id').head(50).sentence_form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Birg9KWWHGY2"},"outputs":[],"source":["p_train_aug = pd.concat([positive, negative_aug, neutral_aug])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"010dmjkolD3N"},"outputs":[],"source":["def reformat_p_binary(df):\n","    p_binary = []\n","    for i, row in df.iterrows():\n","        row.id, row.sentence_form, row.entity_property, row.sentiment\n","        for sentiment in polarity_id_to_name:\n","            if sentiment == row.sentiment:\n","                p_binary.append([row.id, row.sentence_form, '#'.join([row.entity_property, row.sentiment]), tf_name_to_id['True']])\n","            else: \n","                p_binary.append([row.id, row.sentence_form, '#'.join([row.entity_property, sentiment]), tf_name_to_id['False']])\n","    return p_binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffHy0u-5kH9J"},"outputs":[],"source":["p_binary_train_aug = reformat_p_binary(p_train_aug)\n","p_binary_train_aug = pd.DataFrame(p_binary_train_aug, columns=['id', 'sentence_form', 'entity_property', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666184235131,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"TcjTW80FTkU5","outputId":"9487802d-6ab8-431c-e551-a01c0eee48ba"},"outputs":[],"source":["p_binary_train_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue-_eGNSkJpk"},"outputs":[],"source":["p_binary_dev = reformat_p_binary(p_dev)\n","p_binary_dev = pd.DataFrame(p_binary_dev, columns=['id', 'sentence_form', 'entity_property', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"F7JUwP81T7t7","outputId":"99179898-45db-4e4a-c226-8992dd4d7112"},"outputs":[],"source":["p_binary_dev"]},{"cell_type":"markdown","metadata":{"id":"rjaoyM97e9PQ"},"source":["# Counting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"GFroeNPQc-DZ","outputId":"bce5e0c2-0423-4db7-982a-23d5b68b8618"},"outputs":[],"source":["len(ep_train), len(ep_dev), len(p_train), len(p_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"-UNi4guSpiOH","outputId":"59306b7c-8d3a-412d-dd30-b4ab2536de09"},"outputs":[],"source":["len(ep_train), len(ep_dev), len(p_binary_train_aug), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666184237229,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"NB39FuJa_vNH","outputId":"0d143fe1-f1c8-4a3b-bcb4-fbc95fa05433"},"outputs":[],"source":["ep_train = ep_train.drop_duplicates()\n","p_binary_train_aug = p_binary_train_aug.drop_duplicates()\n","ep_dev = ep_dev.drop_duplicates()\n","p_binary_dev = p_binary_dev.drop_duplicates()\n","len(ep_train), len(ep_dev), len(p_binary_train_aug), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9RmADlISEwu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PxiYbGKaFfpE"},"source":["# Export"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1102,"status":"ok","timestamp":1666184351766,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"7AJTuriR2Wsd","outputId":"db6fdeaa-7d72-4bfd-d9fb-13114631b1b1"},"outputs":[],"source":["%cd /content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11\n","\n","# train.to_csv('raw_train.csv', index=False)\n","# dev.to_csv('raw_dev.csv', index=False)\n","# test.to_csv('raw_test.csv', index=False)\n","\n","ep_train.to_csv('ce_train.csv', index=False)\n","p_binary_train_aug.to_csv('pc_binary_train_aug.csv', index=False)\n","ep_dev.to_csv('ce_dev.csv', index=False)\n","p_binary_dev.to_csv('pc_binary_dev.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP_NZbG8fEDN"},"outputs":[],"source":["# emojis = pd.concat([ep_train.sentence_form, p_train.sentence_form, ep_dev.sentence_form, p_dev.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","# emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"executionInfo":{"elapsed":964,"status":"ok","timestamp":1666184397981,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"UyUS26wMZRCu","outputId":"7d5ce0bc-49ac-48dd-9373-21f42f766349"},"outputs":[],"source":["df = pd.read_csv('ce_train.csv')\n","df[df.id == 'nikluge-sa-2022-train-00065']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1666184401423,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"izy5bCjIDTus","outputId":"03dcb49e-d238-4cd4-db55-73771d78026c"},"outputs":[],"source":["df = pd.read_csv('ce_dev.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1666184405643,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"4oEfZBxBK6bG","outputId":"151fe3a5-269b-4968-ca15-a755fa237ae9"},"outputs":[],"source":["df = pd.read_csv('pc_binary_train_aug.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666184410093,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"SEDxWfFhDWDs","outputId":"0726487f-297a-4de9-8738-d24ba8680fe1"},"outputs":[],"source":["df = pd.read_csv('pc_binary_dev.csv')\n","df"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 ('jeonghyeon')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"}}},"nbformat":4,"nbformat_minor":0}
