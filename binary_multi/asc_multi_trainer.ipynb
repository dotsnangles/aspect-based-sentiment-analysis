{"cells":[{"cell_type":"markdown","metadata":{"id":"p8Lb8VDc8Rak"},"source":["# Description"]},{"cell_type":"markdown","metadata":{"id":"9pZhmy9xKmZX"},"source":["# Modules and Global Variables"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6839,"status":"ok","timestamp":1665909475451,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"_v8VXBZdKuUD"},"outputs":[],"source":["from transformers import (\n","    AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, \n","    DefaultDataCollator, DataCollatorWithPadding, \n","    TrainingArguments, Trainer,\n",")\n","\n","from transformers.optimization import (\n","    AdamW, get_linear_schedule_with_warmup,\n","    Adafactor, AdafactorSchedule,\n",")\n","\n","import wandb\n","\n","import datasets\n","import evaluate\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","import numpy as np\n","import pandas as pd\n","import demoji\n","\n","import os\n","import re\n","import random"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1665909475451,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"7t7eYrTJkyCV"},"outputs":[],"source":["### labels\n","\n","ce_labels = ['True', 'False']\n","pc_labels = ['positive', 'negative', 'neutral']\n","\n","labels = pc_labels\n","\n","label2id = {k: i for i, k in enumerate(labels)}\n","id2label = {i: k for i, k in enumerate(labels)}\n","num_labels = len(labels)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2172,"status":"ok","timestamp":1665909477617,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Rohq6E8Lp1x1","outputId":"ca6748a4-5d33-4074-e19a-25642236ee47"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/aspect_based_sentiment_analysis/aspect_sentiment_classification/klue_roberta_base exists.\n","/content/drive/MyDrive/aspect_based_sentiment_analysis/aspect_sentiment_classification/klue_roberta_base/asc_encoder_klue_roberta_base_training.ipynb does not exist.\n","/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v9/pc_train.csv exists.\n","/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v9/pc_dev.csv exists.\n"]}],"source":["### paths and names\n","\n","PROJECT_NAME = 'aspect_sentiment_classification'\n","RUN_ID = 'v5'\n","\n","DATA_V = 'v9'\n","DATA_T = 'pc' # ce or pc\n","AUGMENTATION = False\n","AUG_NAME = 'balanced'\n","\n","model_checkpoint = 'klue/roberta-base'\n","\n","notebook_name = 'asc_encoder_klue_roberta_base_training.ipynb'\n","\n","### fixed\n","\n","model_name = re.sub(r'[/-]', r'_', model_checkpoint)\n","run_name = f'{model_name}_{RUN_ID}'\n","\n","ROOT_PATH = '/content/drive/MyDrive/aspect_based_sentiment_analysis'\n","SAVE_PATH = os.path.join(ROOT_PATH, PROJECT_NAME, model_name)\n","NOTEBOOK_PATH = os.path.join(SAVE_PATH, notebook_name)\n","\n","augornot = f'_{AUG_NAME}' if AUGMENTATION is True else ''\n","TRAIN_DATA_PATH = os.path.join(ROOT_PATH, 'data', DATA_V, f'{DATA_T}_train{augornot}.csv')\n","EVAL_DATA_PATH = os.path.join(ROOT_PATH, 'data', DATA_V, f'{DATA_T}_dev.csv')\n","\n","if os.path.exists(SAVE_PATH):\n","    print(f'{SAVE_PATH} exists.')\n","else:\n","    print(f'{SAVE_PATH} does not exist.')\n","if os.path.exists(NOTEBOOK_PATH):\n","    print(f'{NOTEBOOK_PATH} exists.')\n","else:\n","    print(f'{NOTEBOOK_PATH} does not exist.')\n","if os.path.exists(TRAIN_DATA_PATH):\n","    print(f'{TRAIN_DATA_PATH} exists.')\n","else:\n","    print(f'{TRAIN_DATA_PATH} does not exist.')\n","if os.path.exists(EVAL_DATA_PATH):\n","    print(f'{EVAL_DATA_PATH} exists.')\n","else:\n","    print(f'{EVAL_DATA_PATH} does not exist.')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1665909477617,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"HnBF0kLyk4Z4"},"outputs":[],"source":["### rest of training args\n","\n","report_to=\"wandb\"\n","\n","fp16 = False\n","\n","num_train_epochs = 20\n","batch_size = 8\n","gradient_accumulation_steps = 1\n","\n","optim = 'adamw_torch' # 'adamw_hf'\n","\n","learning_rate = 3e-6 # 5e-5\n","weight_decay = 0.01 # 0\n","adam_epsilon = 1e-8\n","\n","lr_scheduler_type = 'cosine'\n","warmup_ratio = 0\n","\n","save_total_limit = 5\n","\n","load_best_model_at_end = True\n","metric_for_best_model='eval_loss'\n","\n","save_strategy = \"epoch\"\n","evaluation_strategy = \"epoch\"\n","\n","logging_strategy = \"steps\"\n","logging_first_step = True \n","logging_steps = 500"]},{"cell_type":"markdown","metadata":{"id":"yXVsV8jQpreV"},"source":["# WandB Configuration"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"elapsed":6696,"status":"ok","timestamp":1665909484306,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"P0IR11QyPy26","outputId":"975a7139-2ca5-4177-a2f4-78515a0eeb0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find /content/drive/MyDrive/aspect_based_sentiment_analysis/aspect_sentiment_classification/klue_roberta_base/asc_encoder_klue_roberta_base_training.ipynb.\n"]},{"name":"stdout","output_type":"stream","text":["env: WANDB_PROJECT=aspect_sentiment_classification\n","env: WANDB_NOTEBOOK_NAME=/content/drive/MyDrive/aspect_based_sentiment_analysis/aspect_sentiment_classification/klue_roberta_base/asc_encoder_klue_roberta_base_training.ipynb\n","env: WANDB_LOG_MODEL=true\n","env: WANDB_WATCH=all\n"]},{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["%env WANDB_PROJECT={PROJECT_NAME}\n","%env WANDB_NOTEBOOK_NAME={NOTEBOOK_PATH}\n","%env WANDB_LOG_MODEL=true\n","%env WANDB_WATCH=all\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"XSAzFxnH1ozQ"},"source":["# Load Model, Tokenizer, and Collator"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19392,"status":"ok","timestamp":1665909503689,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"v7hpRDtF7ChY","outputId":"7c4a2d90-5002-4212-b227-67e94fc5cf54"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/aspect_based_sentiment_analysis/base_model/klue_roberta_base/v2/klue_roberta_base_mlm/checkpoint-19860 were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/aspect_based_sentiment_analysis/base_model/klue_roberta_base/v2/klue_roberta_base_mlm/checkpoint-19860 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["ckpt = '/content/drive/MyDrive/aspect_based_sentiment_analysis/base_model/klue_roberta_base/v2/klue_roberta_base_mlm/checkpoint-19860'\n","\n","tokenizer = AutoTokenizer.from_pretrained(ckpt)\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    ckpt, label2id=label2id, id2label=id2label, num_labels=num_labels\n",")\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"V-EVcOAQ18dS"},"source":["# Define Metric"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["951f08a17c0b47f7b08d865f7b5d4d83","c8a1ae592cb04de4961d9bb2a0059f7a","93ad0eb1e6f04c24a7e8e7b652d97deb","eb248de28fa94bfc91ab48f10546ccfc","2e41b91ab8284dd1ab7a8517f662f0b8","3e75a95614464a15a238003c0be7410a","ffc1206fdb854734837bcb6254779b1b","9ee48a6d9749428fb7105c1a3535db8b","f586d7ef57dd43c1b882b38ea3da970c","e42de91f0b95420aa1275899325c9d8d","9885fa2b4a19411eb9ed7a940b3c847d","51cb58532f1348c7ba5b24fb907167d3","b2fa1ee7c4b64ce3945018c6c81d7710","8185bff3f84d4fc9887ccc834e54ba73","7439ddb6f7204e79b036aabbe994aea3","fe7452e2f3d64066b83036e61f5b3aca","e42f651d6a634f36814ebd1ebc110731","47bc62cd651f43c7af28a0e8c20affa3","5d6d2167cf7941a7a9516463dd170d8b","4b3f4f696e5640fd906f48805123f5c6","970505e0c2bf48edbf01c720f2f42ef3","8fb2d896bc6c45d3aa22e449056d7879"]},"executionInfo":{"elapsed":5956,"status":"ok","timestamp":1665909509640,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"qtEXOy22Gz8l","outputId":"4c22451f-8f6a-4d23-aabc-a84b6ccef947"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"951f08a17c0b47f7b08d865f7b5d4d83","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51cb58532f1348c7ba5b24fb907167d3","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["accuracy_metric = evaluate.load('accuracy')\n","f1_metric = evaluate.load('f1')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665909509641,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"1d61JHiLEadB"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    \n","    accuracy = accuracy_metric.compute(references=labels, predictions=predictions)['accuracy']\n","    f1_positive, f1_negative, f1_neutral = tuple(f1_metric.compute(references=labels, predictions=predictions, average=None, labels=[0,1,2])['f1'])\n","    f1_macro = f1_metric.compute(references=labels, predictions=predictions, average='macro')['f1']\n","    f1_micro = f1_metric.compute(references=labels, predictions=predictions, average='micro')['f1']\n","    \n","    return {'accuracy': accuracy, 'f1_positive': f1_positive, 'f1_negative': f1_negative, 'f1_neutral': f1_neutral, 'f1_macro': f1_macro, 'f1_micro': f1_micro}"]},{"cell_type":"markdown","metadata":{"id":"EBp5WFGR2JRb"},"source":["# Load Data"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665909509641,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"ryHwmgr7B3Ze"},"outputs":[],"source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"sentence_form\"], examples[\"entity_property\"], truncation=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["7437823ab7354c5cb00f0f528cf176be","2a03a3fa35d946e18a5995ef58c767b6","e3710eb97f584d668db55e51babf2827","0bece1eeb203482f90069fe08ec4e6c2","2d4e59900ce94311bc0b7f0167f2b037","beac82b2a2e748e0b346613965175421","5ca01e00fd7e49d0bd0af80c51e0b242","ece25fb93f3b400faf53f3e37eb775f7","2255752ca95c41f1b94895d4ad8bf8da","4b5aa8beffca4aacafbd94adef06f7e4","8fc21ad82e0d45c895f29eb41ce57257","0f375b2ce49444f793388bab313fcfc5","8af28cfcec37428cb8f63cd5719ecf6f","4121c37f9682492abc0d7f119365ce4d","fcac0eadaa194321ad61abd521b61207","e9253188836a429f83fd726028610c21","eeb718a873c64a96b524b78c46a14b63","75fb25bd46f246f4918d1b8b4b66d3ee","31eeb0b2bb8046258c8b67702631b971","367539ec4825453694354cf14031a471","daa3f413e71d45a5a50fb70e5875f886","744697d2fab644de84b3a3a9bbe26da2"]},"executionInfo":{"elapsed":8314,"status":"ok","timestamp":1665909559029,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"lM9mxmKb2Nah","outputId":"86512156-926b-482f-c96b-1e849bb29739"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7437823ab7354c5cb00f0f528cf176be","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6198 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f375b2ce49444f793388bab313fcfc5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3002 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = pd.read_csv(TRAIN_DATA_PATH)\n","eval_dataset = pd.read_csv(EVAL_DATA_PATH)\n","train_dataset = pd.concat([train_dataset, eval_dataset])\n","train_dataset = datasets.Dataset.from_pandas(train_dataset).shuffle(seed=42)\n","eval_dataset = datasets.Dataset.from_pandas(eval_dataset).shuffle(seed=42)\n","train_dataset = train_dataset.map(preprocess_function, batched=False)\n","eval_dataset = eval_dataset.map(preprocess_function, batched=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1016,"status":"ok","timestamp":1665909565018,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"YUaaH_gi7UFe","outputId":"567b803f-d067-45ba-9211-d988ecab594b"},"outputs":[{"data":{"text/plain":["(6198, 3002)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset), len(eval_dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1665909567901,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"AW4V6ZVKdMRP","outputId":"97ecc0c8-589b-45c8-d282-88403c849f20"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] - 차이오 홈페이지와 오픈마켓 ( 11번가, 지마켓, 옥션 ) 에서 구입이 가능하고 다른 제품과 비교하여 이 제품만이 가지고 있는 의미있는 특이점인 완벽커버가능 ~ ~ [SEP] 제품 전체 # 품질 [SEP]'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["k = random.randrange(len(train_dataset))\n","tokenizer.decode(train_dataset['input_ids'][k])"]},{"cell_type":"markdown","metadata":{"id":"admrPVvW1_Q_"},"source":["# Load Trainer"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1665909569312,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"B0b4moolsNWK"},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=run_name,\n","    run_name=run_name,\n","    report_to=report_to,\n","\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","\n","    optim=optim,\n","\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    adam_epsilon=adam_epsilon,\n","\n","    lr_scheduler_type=lr_scheduler_type,\n","    warmup_ratio=warmup_ratio,\n","\n","    save_total_limit=save_total_limit,\n","\n","    load_best_model_at_end=load_best_model_at_end,\n","    metric_for_best_model=metric_for_best_model,\n","    \n","    save_strategy=save_strategy,\n","    evaluation_strategy=evaluation_strategy,\n","\n","    logging_strategy=logging_strategy,\n","    logging_first_step=logging_first_step, \n","    logging_steps=logging_steps,\n","    \n","    fp16=fp16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1aRCCDR9kVb"},"outputs":[],"source":["# es = EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3765,"status":"ok","timestamp":1665909575771,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"pwe87xkaMEK6"},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n","    # callbacks=[es],\n",")"]},{"cell_type":"markdown","metadata":{"id":"9KNsbWs72BoC"},"source":["# Run Trainer"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2262341,"status":"ok","timestamp":1665911838099,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"tyN8PA8tMFsU","outputId":"524360d5-83c2-4caf-dfaf-f238c96e603e"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: entity_property, sentence_form, id, __index_level_0__. If entity_property, sentence_form, id, __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 6198\n","  Num Epochs = 20\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15500\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdotsnangles\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20221016_083933-3v8as6j5</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/dotsnangles/aspect_sentiment_classification/runs/3v8as6j5\" target=\"_blank\">klue_roberta_base_v3</a></strong> to <a href=\"https://wandb.ai/dotsnangles/aspect_sentiment_classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15500' max='15500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15500/15500 37:07, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Positive</th>\n","      <th>F1 Negative</th>\n","      <th>F1 Neutral</th>\n","      <th>F1 Macro</th>\n","      <th>F1 Micro</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.234400</td>\n","      <td>0.097219</td>\n","      <td>0.977348</td>\n","      <td>0.990641</td>\n","      <td>0.630137</td>\n","      <td>0.000000</td>\n","      <td>0.540259</td>\n","      <td>0.977348</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.128500</td>\n","      <td>0.088459</td>\n","      <td>0.980013</td>\n","      <td>0.991343</td>\n","      <td>0.745763</td>\n","      <td>0.000000</td>\n","      <td>0.579035</td>\n","      <td>0.980013</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.129900</td>\n","      <td>0.104559</td>\n","      <td>0.980680</td>\n","      <td>0.990166</td>\n","      <td>0.857143</td>\n","      <td>0.105263</td>\n","      <td>0.650857</td>\n","      <td>0.980680</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.107300</td>\n","      <td>0.068435</td>\n","      <td>0.986009</td>\n","      <td>0.993869</td>\n","      <td>0.793103</td>\n","      <td>0.513514</td>\n","      <td>0.766829</td>\n","      <td>0.986009</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.087900</td>\n","      <td>0.056873</td>\n","      <td>0.989340</td>\n","      <td>0.994888</td>\n","      <td>0.901961</td>\n","      <td>0.658824</td>\n","      <td>0.851891</td>\n","      <td>0.989340</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.075700</td>\n","      <td>0.055402</td>\n","      <td>0.990340</td>\n","      <td>0.995568</td>\n","      <td>0.888889</td>\n","      <td>0.690476</td>\n","      <td>0.858311</td>\n","      <td>0.990340</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.066900</td>\n","      <td>0.049784</td>\n","      <td>0.992005</td>\n","      <td>0.995907</td>\n","      <td>0.923077</td>\n","      <td>0.772727</td>\n","      <td>0.897237</td>\n","      <td>0.992005</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.050000</td>\n","      <td>0.038866</td>\n","      <td>0.992672</td>\n","      <td>0.996582</td>\n","      <td>0.923077</td>\n","      <td>0.800000</td>\n","      <td>0.906553</td>\n","      <td>0.992672</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.047800</td>\n","      <td>0.033726</td>\n","      <td>0.994670</td>\n","      <td>0.997607</td>\n","      <td>0.923077</td>\n","      <td>0.862745</td>\n","      <td>0.927810</td>\n","      <td>0.994670</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.031400</td>\n","      <td>0.024718</td>\n","      <td>0.995670</td>\n","      <td>0.997949</td>\n","      <td>0.923077</td>\n","      <td>0.900000</td>\n","      <td>0.940342</td>\n","      <td>0.995670</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.026000</td>\n","      <td>0.022991</td>\n","      <td>0.995670</td>\n","      <td>0.998119</td>\n","      <td>0.923077</td>\n","      <td>0.893204</td>\n","      <td>0.938133</td>\n","      <td>0.995670</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.025400</td>\n","      <td>0.020495</td>\n","      <td>0.995670</td>\n","      <td>0.998118</td>\n","      <td>0.923077</td>\n","      <td>0.897196</td>\n","      <td>0.939464</td>\n","      <td>0.995670</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.016800</td>\n","      <td>0.018511</td>\n","      <td>0.996669</td>\n","      <td>0.998461</td>\n","      <td>0.923077</td>\n","      <td>0.933333</td>\n","      <td>0.951624</td>\n","      <td>0.996669</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.009400</td>\n","      <td>0.013139</td>\n","      <td>0.997002</td>\n","      <td>0.998631</td>\n","      <td>0.923077</td>\n","      <td>0.944444</td>\n","      <td>0.955384</td>\n","      <td>0.997002</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.010400</td>\n","      <td>0.014210</td>\n","      <td>0.997335</td>\n","      <td>0.998974</td>\n","      <td>0.923077</td>\n","      <td>0.943396</td>\n","      <td>0.955149</td>\n","      <td>0.997335</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.008100</td>\n","      <td>0.015110</td>\n","      <td>0.997002</td>\n","      <td>0.998632</td>\n","      <td>0.923077</td>\n","      <td>0.942308</td>\n","      <td>0.954672</td>\n","      <td>0.997002</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.008600</td>\n","      <td>0.013375</td>\n","      <td>0.997335</td>\n","      <td>0.998803</td>\n","      <td>0.923077</td>\n","      <td>0.952381</td>\n","      <td>0.958087</td>\n","      <td>0.997335</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.007200</td>\n","      <td>0.012494</td>\n","      <td>0.997335</td>\n","      <td>0.998803</td>\n","      <td>0.923077</td>\n","      <td>0.952381</td>\n","      <td>0.958087</td>\n","      <td>0.997335</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.006100</td>\n","      <td>0.012180</td>\n","      <td>0.997335</td>\n","      <td>0.998803</td>\n","      <td>0.923077</td>\n","      <td>0.952381</td>\n","      <td>0.958087</td>\n","      <td>0.997335</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.007800</td>\n","      <td>0.012125</td>\n","      <td>0.997335</td>\n","      <td>0.998803</td>\n","      <td>0.923077</td>\n","      <td>0.952381</td>\n","      <td>0.958087</td>\n","      <td>0.997335</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-775\n","Configuration saved in klue_roberta_base_v3/checkpoint-775/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-775/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-775/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-775/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-1550\n","Configuration saved in klue_roberta_base_v3/checkpoint-1550/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-1550/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-1550/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-1550/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-2325\n","Configuration saved in klue_roberta_base_v3/checkpoint-2325/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-2325/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-2325/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-2325/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-3100\n","Configuration saved in klue_roberta_base_v3/checkpoint-3100/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-3100/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-3100/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-3100/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-3875\n","Configuration saved in klue_roberta_base_v3/checkpoint-3875/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-3875/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-3875/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-3875/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-4650\n","Configuration saved in klue_roberta_base_v3/checkpoint-4650/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-4650/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-4650/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-4650/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-775] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-5425\n","Configuration saved in klue_roberta_base_v3/checkpoint-5425/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-5425/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-5425/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-5425/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-1550] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-6200\n","Configuration saved in klue_roberta_base_v3/checkpoint-6200/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-6200/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-6200/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-6200/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-2325] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-6975\n","Configuration saved in klue_roberta_base_v3/checkpoint-6975/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-6975/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-6975/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-6975/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-3100] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-7750\n","Configuration saved in klue_roberta_base_v3/checkpoint-7750/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-7750/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-7750/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-7750/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-3875] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-8525\n","Configuration saved in klue_roberta_base_v3/checkpoint-8525/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-8525/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-8525/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-8525/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-4650] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-9300\n","Configuration saved in klue_roberta_base_v3/checkpoint-9300/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-9300/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-9300/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-9300/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-5425] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-10075\n","Configuration saved in klue_roberta_base_v3/checkpoint-10075/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-10075/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-10075/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-10075/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-6200] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-10850\n","Configuration saved in klue_roberta_base_v3/checkpoint-10850/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-10850/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-10850/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-10850/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-6975] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-11625\n","Configuration saved in klue_roberta_base_v3/checkpoint-11625/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-11625/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-11625/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-11625/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-7750] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-12400\n","Configuration saved in klue_roberta_base_v3/checkpoint-12400/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-12400/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-12400/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-12400/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-8525] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-13175\n","Configuration saved in klue_roberta_base_v3/checkpoint-13175/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-13175/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-13175/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-13175/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-9300] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-13950\n","Configuration saved in klue_roberta_base_v3/checkpoint-13950/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-13950/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-13950/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-13950/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-10075] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-14725\n","Configuration saved in klue_roberta_base_v3/checkpoint-14725/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-14725/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-14725/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-14725/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-10850] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, sentence_form, entity_property. If id, sentence_form, entity_property are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3002\n","  Batch size = 8\n","Saving model checkpoint to klue_roberta_base_v3/checkpoint-15500\n","Configuration saved in klue_roberta_base_v3/checkpoint-15500/config.json\n","Model weights saved in klue_roberta_base_v3/checkpoint-15500/pytorch_model.bin\n","tokenizer config file saved in klue_roberta_base_v3/checkpoint-15500/tokenizer_config.json\n","Special tokens file saved in klue_roberta_base_v3/checkpoint-15500/special_tokens_map.json\n","Deleting older checkpoint [klue_roberta_base_v3/checkpoint-11625] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from klue_roberta_base_v3/checkpoint-15500 (score: 0.012125266715884209).\n","Saving model checkpoint to /tmp/tmping857ye\n","Configuration saved in /tmp/tmping857ye/config.json\n","Model weights saved in /tmp/tmping857ye/pytorch_model.bin\n","tokenizer config file saved in /tmp/tmping857ye/tokenizer_config.json\n","Special tokens file saved in /tmp/tmping857ye/special_tokens_map.json\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▂▄▅▆▆▆▇▇▇▇████████</td></tr><tr><td>eval/f1_macro</td><td>▁▂▃▅▆▆▇▇▇███████████</td></tr><tr><td>eval/f1_micro</td><td>▁▂▂▄▅▆▆▆▇▇▇▇████████</td></tr><tr><td>eval/f1_negative</td><td>▁▄▆▅▇▇██████████████</td></tr><tr><td>eval/f1_neutral</td><td>▁▁▂▅▆▆▇▇▇███████████</td></tr><tr><td>eval/f1_positive</td><td>▁▂▁▄▅▅▆▆▇▇▇▇████████</td></tr><tr><td>eval/loss</td><td>▇▇█▅▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▆▁▆▆▁▅▆▆▆▅▆▁▅█▁▅▅▆▅</td></tr><tr><td>eval/samples_per_second</td><td>▃▃█▃▃█▃▃▃▃▃▃█▃▁█▃▃▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▃█▃▃█▃▃▃▃▃▃█▃▁█▃▃▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.99734</td></tr><tr><td>eval/f1_macro</td><td>0.95809</td></tr><tr><td>eval/f1_micro</td><td>0.99734</td></tr><tr><td>eval/f1_negative</td><td>0.92308</td></tr><tr><td>eval/f1_neutral</td><td>0.95238</td></tr><tr><td>eval/f1_positive</td><td>0.9988</td></tr><tr><td>eval/loss</td><td>0.01213</td></tr><tr><td>eval/runtime</td><td>11.223</td></tr><tr><td>eval/samples_per_second</td><td>267.486</td></tr><tr><td>eval/steps_per_second</td><td>33.503</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>15500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0078</td></tr><tr><td>train/total_flos</td><td>2880055915039296.0</td></tr><tr><td>train/train_loss</td><td>0.05215</td></tr><tr><td>train/train_runtime</td><td>2226.4147</td></tr><tr><td>train/train_samples_per_second</td><td>55.677</td></tr><tr><td>train/train_steps_per_second</td><td>6.962</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">klue_roberta_base_v3</strong>: <a href=\"https://wandb.ai/dotsnangles/aspect_sentiment_classification/runs/3v8as6j5\" target=\"_blank\">https://wandb.ai/dotsnangles/aspect_sentiment_classification/runs/3v8as6j5</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221016_083933-3v8as6j5/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.train()\n","wandb.finish()"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":16404,"status":"ok","timestamp":1665911854498,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"njKzTZtir0SC"},"outputs":[],"source":["keep = [\n","    'added_tokens.json',\n","    'config.json',\n","    'pytorch_model.bin',\n","    'special_tokens_map.json',\n","    'tokenizer.json',\n","    'tokenizer_config.json',\n","    'vocab.txt'\n","]\n","\n","ckpts = os.listdir(run_name)\n","for ckpt in ckpts:\n","    ckpt = os.path.join(run_name, ckpt)\n","    for item in os.listdir(ckpt):\n","        if item not in keep:\n","            os.remove(os.path.join(ckpt, item))\n","\n","!cp -r wandb {run_name} {SAVE_PATH}/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1lKTzwHyfXqUlglH4FYVH3lca6fPTxdYY","timestamp":1665143444868},{"file_id":"1SYzd80ssw5Xa_9cqyxAgVHC8lconiyjO","timestamp":1664892294191},{"file_id":"1ol6PhPqw6eJZDA7lphJRBh83aKzP4lLr","timestamp":1664726608470},{"file_id":"1SFw-WNKMa9Ds61AP8W7feAFKz7BAHJ8P","timestamp":1664724312859},{"file_id":"1FZZECpNm4M1SB3O2esRpEC02q2TeHjn7","timestamp":1664720478054}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3.8.0 ('jongmin_nuclear')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.0"},"vscode":{"interpreter":{"hash":"66c58f34006076ce684b86baf9d8f2133ec25c534243b2d921aa9f32735176ac"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0bece1eeb203482f90069fe08ec4e6c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b5aa8beffca4aacafbd94adef06f7e4","placeholder":"​","style":"IPY_MODEL_8fc21ad82e0d45c895f29eb41ce57257","value":" 6198/6198 [00:03&lt;00:00, 1795.93ex/s]"}},"0f375b2ce49444f793388bab313fcfc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8af28cfcec37428cb8f63cd5719ecf6f","IPY_MODEL_4121c37f9682492abc0d7f119365ce4d","IPY_MODEL_fcac0eadaa194321ad61abd521b61207"],"layout":"IPY_MODEL_e9253188836a429f83fd726028610c21"}},"2255752ca95c41f1b94895d4ad8bf8da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a03a3fa35d946e18a5995ef58c767b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beac82b2a2e748e0b346613965175421","placeholder":"​","style":"IPY_MODEL_5ca01e00fd7e49d0bd0af80c51e0b242","value":"100%"}},"2d4e59900ce94311bc0b7f0167f2b037":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e41b91ab8284dd1ab7a8517f662f0b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31eeb0b2bb8046258c8b67702631b971":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"367539ec4825453694354cf14031a471":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e75a95614464a15a238003c0be7410a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4121c37f9682492abc0d7f119365ce4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31eeb0b2bb8046258c8b67702631b971","max":3002,"min":0,"orientation":"horizontal","style":"IPY_MODEL_367539ec4825453694354cf14031a471","value":3002}},"47bc62cd651f43c7af28a0e8c20affa3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b3f4f696e5640fd906f48805123f5c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b5aa8beffca4aacafbd94adef06f7e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51cb58532f1348c7ba5b24fb907167d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2fa1ee7c4b64ce3945018c6c81d7710","IPY_MODEL_8185bff3f84d4fc9887ccc834e54ba73","IPY_MODEL_7439ddb6f7204e79b036aabbe994aea3"],"layout":"IPY_MODEL_fe7452e2f3d64066b83036e61f5b3aca"}},"5ca01e00fd7e49d0bd0af80c51e0b242":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d6d2167cf7941a7a9516463dd170d8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7437823ab7354c5cb00f0f528cf176be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a03a3fa35d946e18a5995ef58c767b6","IPY_MODEL_e3710eb97f584d668db55e51babf2827","IPY_MODEL_0bece1eeb203482f90069fe08ec4e6c2"],"layout":"IPY_MODEL_2d4e59900ce94311bc0b7f0167f2b037"}},"7439ddb6f7204e79b036aabbe994aea3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_970505e0c2bf48edbf01c720f2f42ef3","placeholder":"​","style":"IPY_MODEL_8fb2d896bc6c45d3aa22e449056d7879","value":" 6.77k/6.77k [00:00&lt;00:00, 69.2kB/s]"}},"744697d2fab644de84b3a3a9bbe26da2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75fb25bd46f246f4918d1b8b4b66d3ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8185bff3f84d4fc9887ccc834e54ba73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d6d2167cf7941a7a9516463dd170d8b","max":6771,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b3f4f696e5640fd906f48805123f5c6","value":6771}},"8af28cfcec37428cb8f63cd5719ecf6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeb718a873c64a96b524b78c46a14b63","placeholder":"​","style":"IPY_MODEL_75fb25bd46f246f4918d1b8b4b66d3ee","value":"100%"}},"8fb2d896bc6c45d3aa22e449056d7879":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fc21ad82e0d45c895f29eb41ce57257":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93ad0eb1e6f04c24a7e8e7b652d97deb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ee48a6d9749428fb7105c1a3535db8b","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f586d7ef57dd43c1b882b38ea3da970c","value":4203}},"951f08a17c0b47f7b08d865f7b5d4d83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8a1ae592cb04de4961d9bb2a0059f7a","IPY_MODEL_93ad0eb1e6f04c24a7e8e7b652d97deb","IPY_MODEL_eb248de28fa94bfc91ab48f10546ccfc"],"layout":"IPY_MODEL_2e41b91ab8284dd1ab7a8517f662f0b8"}},"970505e0c2bf48edbf01c720f2f42ef3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9885fa2b4a19411eb9ed7a940b3c847d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ee48a6d9749428fb7105c1a3535db8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2fa1ee7c4b64ce3945018c6c81d7710":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e42f651d6a634f36814ebd1ebc110731","placeholder":"​","style":"IPY_MODEL_47bc62cd651f43c7af28a0e8c20affa3","value":"Downloading builder script: 100%"}},"beac82b2a2e748e0b346613965175421":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8a1ae592cb04de4961d9bb2a0059f7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e75a95614464a15a238003c0be7410a","placeholder":"​","style":"IPY_MODEL_ffc1206fdb854734837bcb6254779b1b","value":"Downloading builder script: 100%"}},"daa3f413e71d45a5a50fb70e5875f886":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3710eb97f584d668db55e51babf2827":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ece25fb93f3b400faf53f3e37eb775f7","max":6198,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2255752ca95c41f1b94895d4ad8bf8da","value":6198}},"e42de91f0b95420aa1275899325c9d8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e42f651d6a634f36814ebd1ebc110731":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9253188836a429f83fd726028610c21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb248de28fa94bfc91ab48f10546ccfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e42de91f0b95420aa1275899325c9d8d","placeholder":"​","style":"IPY_MODEL_9885fa2b4a19411eb9ed7a940b3c847d","value":" 4.20k/4.20k [00:00&lt;00:00, 66.8kB/s]"}},"ece25fb93f3b400faf53f3e37eb775f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb718a873c64a96b524b78c46a14b63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f586d7ef57dd43c1b882b38ea3da970c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcac0eadaa194321ad61abd521b61207":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daa3f413e71d45a5a50fb70e5875f886","placeholder":"​","style":"IPY_MODEL_744697d2fab644de84b3a3a9bbe26da2","value":" 3002/3002 [00:02&lt;00:00, 1421.70ex/s]"}},"fe7452e2f3d64066b83036e61f5b3aca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffc1206fdb854734837bcb6254779b1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
