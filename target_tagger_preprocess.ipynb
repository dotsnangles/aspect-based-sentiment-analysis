{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1666249058207,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"MbTADoPibruA"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ubuntu/anaconda3/envs/jeonghyeon/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from transformers import (\n","    AutoConfig, ElectraTokenizerFast, ElectraForTokenClassification, \n","    DataCollatorForTokenClassification,\n","    TrainingArguments, Trainer,\n",")\n","\n","# from torch.nn import CrossEntropyLoss\n","# loss = CrossEntropyLoss()\n","# loss.ignore_index\n","\n","# import nlpaug.augmenter.char as nac\n","# import nlpaug.augmenter.word as naw\n","# import nlpaug.augmenter.sentence as nas\n","# import nlpaug.flow as nafc\n","# from nlpaug.util import Action\n","\n","# from googletrans import Translator\n","# import translators as ts\n","\n","import re, math, random, json, os\n","from copy import deepcopy\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import Counter\n","\n","from module.preprocess import decorate_form, decorate_acd_pair, decorate_asc_pair, decorate_acd_pair_split, decorate_asc_pair_split\n","from module.utils import count_tags, make_token_classification_pair, remove_props, align_tokens_and_labels, get_filter, generate_token_classification_data, adjust_target\n","import demoji\n","\n","# from cleantext import clean\n","# from pykospacing import Spacing\n","# from hanspell import spell_checker"]},{"cell_type":"markdown","metadata":{},"source":["# Load Raw Data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666249059795,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"NjTs0B-pbxm6"},"outputs":[],"source":["train_json = './dataset/nikluge-sa-2022-train.jsonl'\n","dev_json = './dataset/nikluge-sa-2022-dev.jsonl'\n","test_json = './dataset/nikluge-sa-2022-test.jsonl'\n","\n","train = pd.read_json(train_json, lines=True)\n","dev = pd.read_json(dev_json, lines=True)\n","test = pd.read_json(test_json, lines=True)\n","\n","train = train.drop(2319)\n","dev = dev.drop(1692)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["total = pd.concat([train, dev]).reset_index(drop=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["nikluge-sa-2022-train-00070\n","[['제품 전체#일반', [None, 0, 0], 'neutral'], ['제품 전체#가격', ['', 0, 0], 'neutral']]\n","nikluge-sa-2022-train-01463\n","[['본품#일반', ['향', 0, 1], 'positive'], ['본품#품질', ['', 0, 0], 'positive']]\n","nikluge-sa-2022-train-02516\n","[['본품#품질', ['', 0, 0], 'positive']]\n","nikluge-sa-2022-train-02622\n","[['본품#품질', ['', 0, 0], 'positive']]\n"]}],"source":["for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if annotation[1][0] == '':\n","            print(row.id)\n","            print(row.annotation)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["bool(None)"]},{"cell_type":"markdown","metadata":{},"source":["# Declare Stuff to use"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ov3SPo0UK_2k"},"outputs":[],"source":["### new\n","entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","\n","]\n","more_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","\n","tf_id_to_name = ['True', 'False']\n","tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n","\n","polarity_id_to_name = ['positive', 'negative', 'neutral']\n","polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}"]},{"cell_type":"markdown","metadata":{},"source":["# Fix annotations without a target"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["### Add prefix\n","trgs_prefix = 'Target '\n","trgs_prefix_len = len(trgs_prefix)\n","total['sentence_form'] = trgs_prefix + total['sentence_form']"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Target \n","7\n","Target\n"]}],"source":["print(trgs_prefix)\n","print(trgs_prefix_len)\n","print(trgs_prefix[0:6])\n","trg = 'Target'\n","trg_rng = [0, 6]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["bool('')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["### Fix\n","for idx, row in total.iterrows():\n","    # trg_idx = 0\n","    for idx in range(len(row.annotation)):\n","        if bool(row.annotation[idx][1][0]) == False:\n","            row.annotation[idx][1][0] = trg\n","            row.annotation[idx][1][1] = trg_rng[0]\n","            row.annotation[idx][1][2] = trg_rng[1]\n","        else:\n","            row.annotation[idx][1][1] += trgs_prefix_len\n","            row.annotation[idx][1][2] += trgs_prefix_len\n","    row.annotation = [el for el in row.annotation if el != []]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["### Check\n","for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if annotation[1][0] == None:\n","            print(row)        "]},{"cell_type":"markdown","metadata":{},"source":["# Drop \\# and \\\\xa0"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["total['sentence_form'] = total.sentence_form.str.replace('#', '')\n","total['sentence_form'] = total.sentence_form.str.replace('\\xa0', ' ')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["for _, row in total.iterrows():\n","    for idx in range(len(row.annotation)):\n","        row.annotation[idx][1][0] = row.annotation[idx][1][0].replace('#', '')\n","        row.annotation[idx][1][0] = row.annotation[idx][1][0].replace('\\xa0', ' ')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentence_form</th>\n","      <th>annotation</th>\n","      <th>checker</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [id, sentence_form, annotation, checker]\n","Index: []"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["total['checker'] = total.sentence_form.str.find('#')\n","total[total['checker'] > -1]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentence_form</th>\n","      <th>annotation</th>\n","      <th>checker</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [id, sentence_form, annotation, checker]\n","Index: []"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["total['checker'] = total.sentence_form.str.find('\\xa0')\n","total[total['checker'] > -1]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["### Check\n","for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if '#' in annotation[1][0]:\n","            print(row)        \n","for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if '\\xa0' in annotation[1][0]:\n","            print(row)        "]},{"cell_type":"markdown","metadata":{},"source":["# Drop props that can not be split into train and dev"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# total['checker'] = total.annotation.apply(bool)\n","# total = total[total.checker == True].copy()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# count_tags(total, entity_property_pair)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["filter = [x for x in entity_property_pair if x not in ['본품#인지도', '패키지/구성품#가격']]\n","total = remove_props(total, filter)\n","# count_tags(total, entity_property_pair)"]},{"cell_type":"markdown","metadata":{},"source":["# Before annotation adjustment"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4218\n","Target 팔찌저렴하게 득템했지요~\n","[['제품 전체#가격', ['팔찌', 7, 9], 'positive']]\n","\n","2030\n","Target 나의 애정템 이지요\n","[['제품 전체#일반', ['Target', 0, 6], 'positive']]\n","\n","1545\n","Target 자고나면 어깨와 목이 풀린 느낌이고.. 시원하달까..개운해요^^\n","[['제품 전체#편의성', ['Target', 0, 6], 'positive']]\n","\n","5204\n","Target 신체의 곡선을 빈틈없이 지지해 주는 센스!\n","[['본품#품질', ['Target', 0, 6], 'positive']]\n","\n","4830\n","Target 간편하게 스틱으로 관리할수 있으니 피부관리도 되고 피부미인도 될수 있는데 안하실래요?\n","[['본품#편의성', ['스틱', 12, 14], 'positive']]\n","\n"]}],"source":["random.seed(120)\n","for _ in range(5):\n","    idx = random.randrange(len(total))\n","    print(idx)\n","    print(total.iloc[idx].sentence_form)\n","    print(total.iloc[idx].annotation)\n","    print()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if annotation[1][0] == '':\n","            print(row.id)\n","            print(row.annotation)"]},{"cell_type":"markdown","metadata":{},"source":["# Adjust annotations"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["### adjust annotations\n","for idx, row in total.iterrows():\n","    sentence_form = row.sentence_form\n","    for idx in range(len(row.annotation)):\n","        target = row.annotation[idx][1][0]\n","        target, target_rng = adjust_target(sentence_form, target)\n","\n","        row.annotation[idx][1][0] = target\n","        row.annotation[idx][1][1] = target_rng[0]\n","        row.annotation[idx][1][2] = target_rng[1]\n","\n","    row.annotation = [el for el in row.annotation if el != []]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4218\n","Target 팔찌저렴하게 득템했지요~\n","[['제품 전체#가격', ['팔찌저렴하게', 7, 13], 'positive']]\n","\n","2030\n","Target 나의 애정템 이지요\n","[['제품 전체#일반', ['Target', 0, 6], 'positive']]\n","\n","1545\n","Target 자고나면 어깨와 목이 풀린 느낌이고.. 시원하달까..개운해요^^\n","[['제품 전체#편의성', ['Target', 0, 6], 'positive']]\n","\n","5204\n","Target 신체의 곡선을 빈틈없이 지지해 주는 센스!\n","[['본품#품질', ['Target', 0, 6], 'positive']]\n","\n","4830\n","Target 간편하게 스틱으로 관리할수 있으니 피부관리도 되고 피부미인도 될수 있는데 안하실래요?\n","[['본품#편의성', ['스틱으로', 12, 16], 'positive']]\n","\n"]}],"source":["random.seed(120)\n","for _ in range(5):\n","    idx = random.randrange(len(total))\n","    print(idx)\n","    print(total.iloc[idx].sentence_form)\n","    print(total.iloc[idx].annotation)\n","    print()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["nikluge-sa-2022-train-00265\n","nikluge-sa-2022-train-00619\n","nikluge-sa-2022-train-01286\n","nikluge-sa-2022-train-02412\n","nikluge-sa-2022-train-02511\n","nikluge-sa-2022-dev-00515\n","nikluge-sa-2022-dev-01061\n","nikluge-sa-2022-dev-02365\n"]}],"source":["for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if annotation[1][0] == '':\n","            print(row.id)"]},{"cell_type":"markdown","metadata":{},"source":["# Split"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["total['stratified'] = total.annotation.apply(lambda x: x[0][0])\n","tagger_train, tagger_dev, _, _ = train_test_split(total, total['stratified'], test_size=0.2, random_state=42,  stratify=total['stratified'])\n","tagger_train.reset_index(inplace=True, drop=True)\n","tagger_dev.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# count_tags(tagger_train, entity_property_pair)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# count_tags(tagger_dev, entity_property_pair)"]},{"cell_type":"markdown","metadata":{},"source":["# Generate token classification pairs"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["train_split_inputs, train_split_labels = generate_token_classification_data(tagger_train)\n","dev_split_inputs, dev_split_labels = generate_token_classification_data(tagger_dev)\n","tagger_train['split_form'], tagger_train['split_label'] = train_split_inputs, train_split_labels\n","tagger_dev['split_form'], tagger_dev['split_label'] = dev_split_inputs, dev_split_labels"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize and Align"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["target_tagger_labels = ['Other', 'TRG_B', 'TRG_I']\n","labels = target_tagger_labels\n","label2id = {k: i for i, k in enumerate(labels)}\n","id2label = {i: k for i, k in enumerate(labels)}\n","num_labels = len(labels)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n","- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["({'Other': 0, 'TRG_B': 1, 'TRG_I': 2}, {0: 'Other', 1: 'TRG_B', 2: 'TRG_I'}, 3)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model_checkpoint = 'monologg/koelectra-base-v3-discriminator'\n","tokenizer = ElectraTokenizerFast.from_pretrained(model_checkpoint)\n","model = ElectraForTokenClassification.from_pretrained(\n","    model_checkpoint, label2id=label2id, id2label=id2label, num_labels=num_labels\n",")\n","model.config.label2id, model.config.id2label, model.num_labels"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["35000\n","\n","\n","\n","3060\n","35254\n"]},{"data":{"text/plain":["Embedding(35254, 768)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["### new\n","entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","]\n","sentiments = ['positive', 'negative', 'neutral']\n","target = ['Target']\n","special_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","emojis = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))\n","ep_labels = pd.Series(entity_property_pair, name='sentence_form', copy=True)\n","\n","tokens2add = special_tokens + emojis\n","# tokens2add = special_tokens + emojis + entity_property_pair + sentiments + target\n","\n","print(len(tokenizer))\n","tokenizer_train_data = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame().drop_duplicates()\n","tokenizer_train_data = tokenizer_train_data.sentence_form.to_list()\n","new_tokenizer = tokenizer.train_new_from_iterator(tokenizer_train_data, vocab_size=1)\n","new_tokens = set(list(new_tokenizer.vocab.keys()) + tokens2add) - set(tokenizer.vocab.keys())\n","tokenizer.add_tokens(list(new_tokens))\n","print(len(new_tokenizer))\n","print(len(tokenizer))\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["input_tokens_list, labels = align_tokens_and_labels(tagger_train, tokenizer)\n","tagger_train['input_tokens_list'], tagger_train['labels'] = input_tokens_list, labels\n","input_tokens_list, labels = align_tokens_and_labels(tagger_dev, tokenizer)\n","tagger_dev['input_tokens_list'], tagger_dev['labels'] = input_tokens_list, labels"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Empty DataFrame\n","Columns: [id, sentence_form, annotation, checker, stratified, split_form, split_label, input_tokens_list, labels]\n","Index: []\n","Empty DataFrame\n","Columns: [id, sentence_form, annotation, checker, stratified, split_form, split_label, input_tokens_list, labels]\n","Index: []\n"]}],"source":["tagger_train['checker'] = tagger_train.input_tokens_list.apply(len) == tagger_train.labels.apply(len)\n","print(tagger_train[tagger_train.checker == False])\n","tagger_dev['checker'] = tagger_dev.input_tokens_list.apply(len) == tagger_dev.labels.apply(len)\n","print(tagger_dev[tagger_dev.checker == False])"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['제품 전체#일반', ['히말라야 스파키트,', 24, 34], 'positive']]\n","Target 그 때 사용해보고 완전 반했던 히말라야 스파키트, 요즘 집에서 매일 사용 중이에요 ㅎㅎ\n","['[CLS]', 'T', '##ar', '##ge', '##t', '그', '때', '사용', '##해', '##보', '##고', '완전', '반', '##했', '##던', '히말라야', '스파', '##키', '##트', ',', '요즘', '집', '##에', '##서', '매일', '사용', '중이', '##에', '##요', 'ㅎㅎ', '[SEP]']\n","[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n","\n","[['본품#품질', ['Target', 0, 6], 'positive']]\n","Target 진짜 운동할때 땀이 엄청나고~.\n","['[CLS]', 'T', '##ar', '##ge', '##t', '진짜', '운동', '##할', '##때', '땀', '##이', '엄청나', '##고', '~', '.', '[SEP]']\n","[-100, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"]}],"source":["idx = random.randrange(len(tagger_train))\n","print(tagger_train.iloc[idx].annotation)\n","print(tagger_train.iloc[idx].sentence_form)\n","print(tagger_train.iloc[idx].input_tokens_list)\n","print(tagger_train.iloc[idx].labels)\n","print()\n","idx = random.randrange(len(tagger_dev))\n","print(tagger_dev.iloc[idx].annotation)\n","print(tagger_dev.iloc[idx].sentence_form)\n","print(tagger_dev.iloc[idx].input_tokens_list)\n","print(tagger_dev.iloc[idx].labels)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["[['패키지/구성품#디자인', ['패지키', 12, 15], 'positive'], ['본품#품질', ['Target', 0, 6], 'positive']]\n","toks = ['[CLS]', 'T', '##ar', '##ge', '##t', '거기', '##다가', '패', '##지', '##키', '까지', '예쁘', '##고', '계', '##면', '##활', '##성', '##제', '##도', '없', '##고', '.', '.', '[SEP]']\n","labs = [-100, 1, 2, 2, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["[['T', '##ar', '##ge', '##t'], ['패', '##지', '##키']]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["starts = list({k:v for k, v in enumerate(labs) if v == 1}.keys())\n","targets = []\n","for start in starts:\n","    target = [toks[start]]\n","    for tok, lab in zip(toks[start+1:], labs[start+1:]):\n","        if lab != 2:\n","            break\n","        else:\n","            target.append(tok)\n","    targets.append(target)\n","targets"]},{"cell_type":"markdown","metadata":{},"source":["# Save Tagger Data"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["./dataset/uncleaned_v11\n"]}],"source":["DATA_V = 'uncleaned_v11'\n","save_path = f'./dataset/{DATA_V}'\n","print(save_path)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["('./dataset/uncleaned_v11/tokenizer/tokenizer_config.json',\n"," './dataset/uncleaned_v11/tokenizer/special_tokens_map.json',\n"," './dataset/uncleaned_v11/tokenizer/vocab.txt',\n"," './dataset/uncleaned_v11/tokenizer/added_tokens.json',\n"," './dataset/uncleaned_v11/tokenizer/tokenizer.json')"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(os.path.join(save_path, 'tokenizer'))"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!mkdir -p {save_path}\n","\n","train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","tagger_train.to_json(f'{save_path}/tagger_train.json')\n","tagger_dev.to_json(f'{save_path}/tagger_dev.json')\n","\n","# ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","# ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","\n","# p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","# p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Filter entity_property_pair and Drop rows accordingly"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FILTER_MODE = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if FILTER_MODE == True:\n","    filter = get_filter()\n","    train = remove_props(train, filter)\n","    dev = remove_props(dev, filter)\n","len(train), len(dev)"]},{"cell_type":"markdown","metadata":{"id":"hkFQP8GW4SXT"},"source":["# Preprocess"]},{"cell_type":"markdown","metadata":{},"source":["## Cleansing"]},{"cell_type":"markdown","metadata":{"id":"PII4v9rJLzaV"},"source":["### Before"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666249062532,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"oBhyEPcfmFdB","outputId":"362f9678-bda2-4ded-f372-23c0e9fdd36d"},"outputs":[],"source":["# for el in train.sample(n=5).sentence_form:\n","#     print(el)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5570,"status":"ok","timestamp":1666249070279,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"zxaftwE_uq9w"},"outputs":[],"source":["# train.sentence_form = train.sentence_form.apply(preprocess)\n","# dev.sentence_form = dev.sentence_form.apply(preprocess)\n","# test.sentence_form = test.sentence_form.apply(preprocess)\n","# total = pd.concat([train, dev])"]},{"cell_type":"markdown","metadata":{"id":"B28qDpz-L47B"},"source":["### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1666249157277,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"TmQV4tDyycbX"},"outputs":[],"source":["# case = total.sentence_form.str.contains('r[^A-Za-z0-9가-힣\\s]+', case=False, flags=0, na=None, regex=True)\n","# for e in total[case].sentence_form:\n","#     print(e)"]},{"cell_type":"markdown","metadata":{"id":"ymHR_z-7L3LW"},"source":["### After"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666249070568,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"I94k-mG7s_q5","outputId":"9e82bf97-1530-4783-9fb3-80c82299a3c3"},"outputs":[],"source":["# for i, row in total[['id', 'sentence_form']].sample(n=5).iterrows():\n","#     print(row.id, '\\t', row.sentence_form)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# total['check'] = total.sentence_form.str.find('OO')\n","# for row in total[total.check > -1].sentence_form:\n","#     print(row)\n","#     break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# total"]},{"cell_type":"markdown","metadata":{"id":"25AnAYGke6BQ"},"source":["## Reformat"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666184223688,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"cRZR6lTRED9s","outputId":"08f43287-453a-4377-fe1b-e7d8b3fe6fd8"},"outputs":[],"source":["len(entity_property_pair)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["decorate_form, decorate_acd_pair, decorate_asc_pair, decorate_acd_pair_split, decorate_asc_pair_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9K9UG5aybovX"},"outputs":[],"source":["def reformat(df):\n","    ep =[]\n","    p = []\n","    for index, row in df.iterrows():\n","        utterance = row.sentence_form\n","        id = row.id\n","        \n","        form = utterance\n","        # form = decorate_form(utterance)\n","\n","        for pair in entity_property_pair:\n","            isPairInOpinion = False\n","            if pd.isna(utterance):\n","                break\n","            for annotation in row.annotation:\n","                entity_property = annotation[0]\n","                sentiment = annotation[2]\n","                if entity_property == pair:\n","                    \n","                    acd_pair = entity_property\n","                    # acd_pair = decorate_acd_pair(entity_property)\n","                    # acd_pair = decorate_acd_pair_split(entity_property)\n","                    \n","                    ep_append = [id, form, acd_pair, tf_name_to_id['True']]\n","                    ep.append(ep_append)\n","                    p.append([id, utterance, entity_property, sentiment])\n","                    isPairInOpinion = True\n","                    break\n","            if isPairInOpinion is False:\n","                \n","                acd_pair = pair\n","                # acd_pair = decorate_acd_pair(pair)\n","                # acd_pair = decorate_acd_pair_split(pair)\n","                \n","                ep_append = [id, form, acd_pair, tf_name_to_id['False']]\n","                ep.append(ep_append)\n","    return ep, p"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reformat_p_binary(df):\n","    p_binary = []\n","    for i, row in df.iterrows():\n","        row.id, row.form, row.pair, row.sentiment\n","        \n","        form = row.form\n","        # form = decorate_form(row.form)\n","        \n","        for sentiment in polarity_id_to_name:\n","            if sentiment == row.sentiment:\n","\n","                asc_pair = '#'.join([row.pair, row.sentiment])\n","                # asc_pair = decorate_asc_pair(row.pair, row.sentiment)\n","                # asc_pair = decorate_asc_pair_split(row.pair, row.sentiment)\n","\n","                p_binary_append = [row.id, form, asc_pair, tf_name_to_id['True']]\n","                p_binary.append(p_binary_append)\n","            else:\n","\n","                asc_pair = '#'.join([row.pair, sentiment])\n","                # asc_pair = decorate_asc_pair(row.pair, sentiment)\n","                # asc_pair = decorate_asc_pair_split(row.pair, sentiment)\n","\n","                p_binary_append = [row.id, form, asc_pair, tf_name_to_id['False']]\n","                p_binary.append(p_binary_append)\n","    return p_binary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(train), len(dev)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unJ9pUXwYTTm"},"outputs":[],"source":["ep_train, p_train = reformat(train)\n","ep_dev, p_dev = reformat(dev)\n","\n","ep_train = pd.DataFrame(ep_train, columns=['id', 'form', 'pair', 'labels'])\n","ep_dev = pd.DataFrame(ep_dev, columns=['id', 'form', 'pair', 'labels'])\n","\n","p_train = pd.DataFrame(p_train, columns=['id', 'form', 'pair', 'sentiment'])\n","p_dev = pd.DataFrame(p_dev, columns=['id', 'form', 'pair', 'sentiment'])\n","\n","len(ep_train), len(ep_dev), len(p_train), len(p_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["p_binary_train = reformat_p_binary(p_train)\n","p_binary_train = pd.DataFrame(p_binary_train, columns=['id', 'form', 'pair', 'labels'])\n","\n","p_binary_dev = reformat_p_binary(p_dev)\n","p_binary_dev = pd.DataFrame(p_binary_dev, columns=['id', 'form', 'pair', 'labels'])\n","\n","len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ep_train.sort_values(['id', 'labels'], inplace=True)\n","# ep_dev.sort_values(['id', 'labels'], inplace=True)\n","# p_binary_train.sort_values(['id', 'labels'], inplace=True, ascending=[True, True])\n","# p_binary_dev.sort_values(['id', 'labels'], inplace=True, ascending=[True, True])"]},{"cell_type":"markdown","metadata":{"id":"rjaoyM97e9PQ"},"source":["### Counting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"GFroeNPQc-DZ","outputId":"bce5e0c2-0423-4db7-982a-23d5b68b8618"},"outputs":[],"source":["print('binary_multi: ', end=''), print(len(ep_train), len(ep_dev), len(p_train), len(p_dev))\n","print('binary_binary: ', end=''), print(len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev))\n","ep_train = ep_train.drop_duplicates()\n","ep_dev = ep_dev.drop_duplicates()\n","p_train = p_train.drop_duplicates()\n","p_dev = p_dev.drop_duplicates()\n","p_binary_train = p_binary_train.drop_duplicates()\n","p_binary_dev = p_binary_dev.drop_duplicates()\n","print('\\nafter drop_duplicates\\n')\n","print('binary_multi: ', end=''), print(len(ep_train), len(ep_dev), len(p_train), len(p_dev))\n","print('binary_binary: ', end=''), print(len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev))"]},{"cell_type":"markdown","metadata":{},"source":["### Validate Here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = ep_train\n","for idx, row in df.iterrows():\n","    print(row.id, '\\n',\n","          row.form, '\\n',\n","          row.pair, '\\n',\n","          row.labels,  '\\n',)\n","    if idx == 49:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["### Save"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_V = 'uncleaned_v6'\n","save_path = f'./dataset/{DATA_V}'\n","print(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir -p {save_path}\n","\n","train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","\n","p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Additional Length Test If Needed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ep_train, ep_dev, p_binary_train, p_binary_dev"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_checkpoint = 'snunlp/KR-ELECTRA-discriminator'\n","\n","train_path = f'./dataset/{DATA_V}/raw_train.csv'\n","dev_path = f'./dataset/{DATA_V}/raw_dev.csv'\n","test_path = f'./dataset/{DATA_V}/raw_test.csv'\n","train = pd.read_csv(train_path)\n","dev = pd.read_csv(dev_path)\n","test = pd.read_csv(test_path)\n","\n","### new\n","entity_property_pair = [\n","    '본품#가격', '본품#다양성', '본품#디자인', '본품#인지도', '본품#일반', '본품#편의성', '본품#품질',\n","    '브랜드#가격', '브랜드#디자인', '브랜드#인지도', '브랜드#일반', '브랜드#품질',\n","    '제품 전체#가격', '제품 전체#다양성', '제품 전체#디자인', '제품 전체#인지도', '제품 전체#일반', '제품 전체#편의성', '제품 전체#품질',\n","    '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#디자인', '패키지/구성품#일반', '패키지/구성품#편의성', '패키지/구성품#품질'\n","]\n","special_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","emojis = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))\n","ep_labels = pd.Series(entity_property_pair, name='sentence_form', copy=True)\n","\n","tokens2add = special_tokens + emojis\n","\n","tokenizer = ElectraTokenizerFast.from_pretrained(model_checkpoint)\n","print(len(tokenizer))\n","tokenizer_train_data = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame().drop_duplicates()\n","tokenizer_train_data = tokenizer_train_data.sentence_form.to_list()\n","new_tokenizer = tokenizer.train_new_from_iterator(tokenizer_train_data, vocab_size=1)\n","new_tokens = set(list(new_tokenizer.vocab.keys()) + tokens2add) - set(tokenizer.vocab.keys())\n","tokenizer.add_tokens(list(new_tokens))\n","print(len(new_tokenizer))\n","print(len(tokenizer))\n","# model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ep_train, ep_dev, p_binary_train, p_binary_dev\n","len_counter = []\n","for df in [ep_train, ep_dev, p_binary_train, p_binary_dev]:\n","    for idx, row in df.iterrows():\n","        len_counter.append(len(tokenizer(row[\"form\"], row[\"pair\"], truncation=True).input_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max(len_counter)"]},{"cell_type":"markdown","metadata":{},"source":["### done here."]},{"cell_type":"markdown","metadata":{},"source":["## Save Files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save_path = './dataset/cleaned_v1'\n","\n","# train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","# dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","# test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","# ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","# ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","# p_train.to_csv(f'{save_path}/pc_train.csv', index=False)\n","# p_dev.to_csv(f'{save_path}/pc_dev.csv', index=False)\n","# p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","# p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"jAr5_Zc6HHQV"},"source":["# ASC Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJxucvEoipu2"},"outputs":[],"source":["model_checkpoint = '/content/drive/MyDrive/aspect_based_sentiment_analysis/base_model/klue_roberta_base/v2/klue_roberta_base_mlm/checkpoint-19860'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOg2ROQQi2OQ"},"outputs":[],"source":["sTokens = tokenizer.all_special_tokens\n","\n","def delTokens(sent):\n","    sent = sent.split(' ')\n","    temp = []\n","    for e in sent:\n","        if e not in sTokens:\n","            temp.append(e)\n","    return ' '.join(temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abyMklzeI81o"},"outputs":[],"source":["positive, negative, neutral = p_train[p_train.sentiment == 'positive'], p_train[p_train.sentiment == 'negative'], p_train[p_train.sentiment == 'neutral']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666184227323,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Vl33p6WhHmOj","outputId":"ff54ba6a-ccd2-4fb2-f716-03a4469cc43d"},"outputs":[],"source":["len(positive), len(negative), len(neutral)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666184227323,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Tt1lAdOUHDyq","outputId":"8c17fd2d-5a8b-4f7a-ac7b-8a69604a8281"},"outputs":[],"source":["(58 * 3) * 4 * 3, (95 * 3) * 4 * 2 # bt ri rr"]},{"cell_type":"markdown","metadata":{"id":"nI6TIyYLINET"},"source":["Back Translation / Random Insertion / Random Replacement / Random Swap / Random Deletion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeypyouVr6Vk"},"outputs":[],"source":["def backTrans(text):\n","    aug1 = ts.papago(text, sleep_seconds=5, from_language='ko', to_language='en')\n","    aug1 = ts.papago(aug1, sleep_seconds=5, from_language='en', to_language='ko')\n","\n","    aug2 = ts.papago(text, sleep_seconds=5, from_language='ko', to_language='ja')\n","    aug2 = ts.papago(aug2, sleep_seconds=5, from_language='ja', to_language='ko')\n","\n","    return [aug1, aug2]\n","\n","def randomInsert(num, sample, device):\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path=model_checkpoint, action=\"insert\", model_type='bert', top_k=5, aug_p=0.3, aug_min=1, aug_max=1, device=device)\n","\n","    aug_result = aug.augment(sample, n=num, num_thread=12)\n","    aug_result = list(map(delTokens, aug_result))\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomReplace(num, sample, device):\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path=model_checkpoint, action=\"insert\", model_type='bert', top_k=5, aug_p=0.3, aug_min=1, aug_max=1, device=device)\n","\n","    aug_result = aug.augment(sample, n=num, num_thread=12)\n","    aug_result = list(map(delTokens, aug_result))\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomSwap(num, sample):\n","    aug = naw.RandomWordAug(action='swap', aug_min=1, aug_max=1, aug_p=0.3)    \n","    aug_result = aug.augment(sample, n=num, num_thread=2)\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomSplit(num, sample):\n","    aug = naw.SplitAug(aug_min=1, aug_max=1, aug_p=0.3, min_char=3)\n","    aug_result = aug.augment(sample, n=num, num_thread=2)\n","    aug_result = list(set(aug_result))\n","    return aug_result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666184227324,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"S61gxehDqAsf","outputId":"a5a79918-c784-41e5-a0f2-7f0006086908"},"outputs":[],"source":["(58 * 3) * 5 * 4, (95 * 3) * 4 * 3 # bt ri rr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QVPawzK3Uqt"},"outputs":[],"source":["def backtransRoutine(data2augment, output_path):\n","    print('back translation started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = backTrans(row[1])\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(len(f'back translation finished.\\ncurrent count: {len(data2augment)}'))\n","\n","    data_aug = pd.DataFrame(data2augment, columns=['id', 'sentence_form', 'entity_property', 'sentiment'])\n","    data_aug.to_csv(f'{output_path}', index=False)\n","\n","    return data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ab6cjfGYqpJG"},"outputs":[],"source":["import os\n","\n","def edaRoutine(data2augment, ri, rr, output_path):\n","    print(f'current count: {len(data2augment)}')\n","    print('random insertion started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = randomInsert(ri, row[1], 'cuda')\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(f'random insertion finished.\\ncurrent count: {len(data2augment)}')\n","\n","    print('random replacement started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = randomReplace(rr, row[1], 'cuda')\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(f'random replacement finished.\\ncurrent count: {len(data2augment)}')\n","\n","    print('random swap and split started.')\n","    while len(data2augment) < len(positive):\n","        temp = []\n","        k = random.randrange(len(negative))\n","        id, text, entity, sentiment = data2augment[k]\n","\n","        selector = random.randint(0,1)\n","        if selector == 0:\n","            augs = randomSwap(1, text)\n","            for aug in augs:\n","                if aug != '' and aug != text:\n","                    new = [id, aug, entity, sentiment]\n","                    if new not in data2augment:\n","                        temp.append(new)\n","            data2augment.extend(temp)\n","        else:\n","            augs = randomSplit(1, text)\n","            for aug in augs:\n","                if aug != '' and aug != text:\n","                    new = [id, aug, entity, sentiment]\n","                    if new not in data2augment:\n","                        temp.append(new)\n","            data2augment.extend(temp)\n","        if len(data2augment)%25 == 0:\n","            print(f'random swap and split in progress.\\ncurrent count: {len(data2augment)}')\n","\n","    print(f'whole augmentation routine finished.\\ntotal count: {len(data2augment)}')\n","\n","    data_aug = pd.DataFrame(data2augment, columns=['id', 'sentence_form', 'entity_property', 'sentiment'])\n","    data_aug.to_csv(f'{output_path}', index=False)\n","\n","    return data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmx-J4bu1JuU"},"outputs":[],"source":["### negative\n","# # back translation\n","\n","# data2augment = negative.values.tolist()\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'negative_bt.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# negative_bt = backtransRoutine(data2augment, output_path)\n","negative_bt = pd.read_csv(output_path)\n","negative_bt = negative_bt.values.tolist()\n","# RI / RR\n","\n","ri = 4 # times - 1\n","rr = 3 # times - 1\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'negative_aug.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# negative_aug = edaRoutine(negative_bt, ri, rr, output_path)\n","negative_aug = pd.read_csv(output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qu5fuqt5Q6AY"},"outputs":[],"source":["negative_aug\n","negative_aug = negative_aug.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GUBfkU5QIZD"},"outputs":[],"source":["# negative_aug.sample(n=15, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eP_iTJrASqcr"},"outputs":[],"source":["# negative_aug.sort_values('id').head(50).sentence_form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XROBqEb71MmR"},"outputs":[],"source":["### neutral\n","# back translation\n","\n","# data2augment = neutral.values.tolist()\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'neutral_bt.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# neutral_bt = backtransRoutine(data2augment, output_path)\n","neutral_bt = pd.read_csv(output_path)\n","neutral_bt = neutral_bt.values.tolist()\n","\n","# RI / RR\n","\n","ri = 3 # times - 1\n","rr = 2 # times - 1\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'neutral_aug.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# neutral_aug = edaRoutine(neutral_bt, ri, rr, output_path)\n","neutral_aug = pd.read_csv(output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MS1g9SKERC1T"},"outputs":[],"source":["neutral_aug\n","neutral_aug = neutral_aug.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC38BqB7RFMr"},"outputs":[],"source":["# neutral_aug.sample(n=15, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v5pqYIlRheZ"},"outputs":[],"source":["# neutral_aug.sort_values('id').head(50).sentence_form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Birg9KWWHGY2"},"outputs":[],"source":["p_train_aug = pd.concat([positive, negative_aug, neutral_aug])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"010dmjkolD3N"},"outputs":[],"source":["def reformat_p_binary(df):\n","    p_binary = []\n","    for i, row in df.iterrows():\n","        row.id, row.sentence_form, row.entity_property, row.sentiment\n","        for sentiment in polarity_id_to_name:\n","            if sentiment == row.sentiment:\n","                p_binary.append([row.id, row.sentence_form, '#'.join([row.entity_property, row.sentiment]), tf_name_to_id['True']])\n","            else: \n","                p_binary.append([row.id, row.sentence_form, '#'.join([row.entity_property, sentiment]), tf_name_to_id['False']])\n","    return p_binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffHy0u-5kH9J"},"outputs":[],"source":["p_binary_train_aug = reformat_p_binary(p_train_aug)\n","p_binary_train_aug = pd.DataFrame(p_binary_train_aug, columns=['id', 'sentence_form', 'entity_property', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666184235131,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"TcjTW80FTkU5","outputId":"9487802d-6ab8-431c-e551-a01c0eee48ba"},"outputs":[],"source":["p_binary_train_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue-_eGNSkJpk"},"outputs":[],"source":["p_binary_dev = reformat_p_binary(p_dev)\n","p_binary_dev = pd.DataFrame(p_binary_dev, columns=['id', 'sentence_form', 'entity_property', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"F7JUwP81T7t7","outputId":"99179898-45db-4e4a-c226-8992dd4d7112"},"outputs":[],"source":["p_binary_dev"]},{"cell_type":"markdown","metadata":{"id":"rjaoyM97e9PQ"},"source":["# Counting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"GFroeNPQc-DZ","outputId":"bce5e0c2-0423-4db7-982a-23d5b68b8618"},"outputs":[],"source":["len(ep_train), len(ep_dev), len(p_train), len(p_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"-UNi4guSpiOH","outputId":"59306b7c-8d3a-412d-dd30-b4ab2536de09"},"outputs":[],"source":["len(ep_train), len(ep_dev), len(p_binary_train_aug), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666184237229,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"NB39FuJa_vNH","outputId":"0d143fe1-f1c8-4a3b-bcb4-fbc95fa05433"},"outputs":[],"source":["ep_train = ep_train.drop_duplicates()\n","p_binary_train_aug = p_binary_train_aug.drop_duplicates()\n","ep_dev = ep_dev.drop_duplicates()\n","p_binary_dev = p_binary_dev.drop_duplicates()\n","len(ep_train), len(ep_dev), len(p_binary_train_aug), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9RmADlISEwu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PxiYbGKaFfpE"},"source":["# Export"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1102,"status":"ok","timestamp":1666184351766,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"7AJTuriR2Wsd","outputId":"db6fdeaa-7d72-4bfd-d9fb-13114631b1b1"},"outputs":[],"source":["%cd /content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11\n","\n","# train.to_csv('raw_train.csv', index=False)\n","# dev.to_csv('raw_dev.csv', index=False)\n","# test.to_csv('raw_test.csv', index=False)\n","\n","ep_train.to_csv('ce_train.csv', index=False)\n","p_binary_train_aug.to_csv('pc_binary_train_aug.csv', index=False)\n","ep_dev.to_csv('ce_dev.csv', index=False)\n","p_binary_dev.to_csv('pc_binary_dev.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP_NZbG8fEDN"},"outputs":[],"source":["# emojis = pd.concat([ep_train.sentence_form, p_train.sentence_form, ep_dev.sentence_form, p_dev.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","# emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"executionInfo":{"elapsed":964,"status":"ok","timestamp":1666184397981,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"UyUS26wMZRCu","outputId":"7d5ce0bc-49ac-48dd-9373-21f42f766349"},"outputs":[],"source":["df = pd.read_csv('ce_train.csv')\n","df[df.id == 'nikluge-sa-2022-train-00065']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1666184401423,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"izy5bCjIDTus","outputId":"03dcb49e-d238-4cd4-db55-73771d78026c"},"outputs":[],"source":["df = pd.read_csv('ce_dev.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1666184405643,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"4oEfZBxBK6bG","outputId":"151fe3a5-269b-4968-ca15-a755fa237ae9"},"outputs":[],"source":["df = pd.read_csv('pc_binary_train_aug.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666184410093,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"SEDxWfFhDWDs","outputId":"0726487f-297a-4de9-8738-d24ba8680fe1"},"outputs":[],"source":["df = pd.read_csv('pc_binary_dev.csv')\n","df"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 ('jeonghyeon')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"}}},"nbformat":4,"nbformat_minor":0}
