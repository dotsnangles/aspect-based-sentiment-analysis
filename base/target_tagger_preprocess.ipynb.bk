{"cells":[{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1666249058207,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"MbTADoPibruA"},"outputs":[],"source":["from transformers import (\n","    AutoConfig, ElectraTokenizerFast, ElectraForTokenClassification, \n","    DataCollatorForTokenClassification,\n","    TrainingArguments, Trainer,\n",")\n","\n","# from torch.nn import CrossEntropyLoss\n","# loss = CrossEntropyLoss()\n","# loss.ignore_index\n","\n","# import nlpaug.augmenter.char as nac\n","# import nlpaug.augmenter.word as naw\n","# import nlpaug.augmenter.sentence as nas\n","# import nlpaug.flow as nafc\n","# from nlpaug.util import Action\n","\n","# from googletrans import Translator\n","# import translators as ts\n","\n","import re, math, random, json\n","from copy import deepcopy\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import Counter\n","\n","from module.preprocess import decorate_form, decorate_acd_pair, decorate_asc_pair, decorate_acd_pair_split, decorate_asc_pair_split\n","from module.utils import count_tags, make_token_classification_pair, remove_props, align_tokens_and_labels, get_filter, generate_token_classification_data\n","import demoji\n","\n","# from cleantext import clean\n","# from pykospacing import Spacing\n","# from hanspell import spell_checker"]},{"cell_type":"markdown","metadata":{},"source":["# Load Raw Data"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666249059795,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"NjTs0B-pbxm6"},"outputs":[],"source":["train_json = './dataset/nikluge-sa-2022-train.jsonl'\n","dev_json = './dataset/nikluge-sa-2022-dev.jsonl'\n","test_json = './dataset/nikluge-sa-2022-test.jsonl'\n","\n","train = pd.read_json(train_json, lines=True)\n","dev = pd.read_json(dev_json, lines=True)\n","test = pd.read_json(test_json, lines=True)\n","\n","train = train.drop(2319)\n","dev = dev.drop(1692)"]},{"cell_type":"markdown","metadata":{},"source":["# Declare Stuff to use"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ov3SPo0UK_2k"},"outputs":[],"source":["### new\n","entity_property_pair = [\n","    'ë³¸í’ˆ#ê°€ê²©', 'ë³¸í’ˆ#ë‹¤ì–‘ì„±', 'ë³¸í’ˆ#ë””ìì¸', 'ë³¸í’ˆ#ì¸ì§€ë„', 'ë³¸í’ˆ#ì¼ë°˜', 'ë³¸í’ˆ#í¸ì˜ì„±', 'ë³¸í’ˆ#í’ˆì§ˆ',\n","    'ë¸Œëœë“œ#ê°€ê²©', 'ë¸Œëœë“œ#ë””ìì¸', 'ë¸Œëœë“œ#ì¸ì§€ë„', 'ë¸Œëœë“œ#ì¼ë°˜', 'ë¸Œëœë“œ#í’ˆì§ˆ',\n","    'ì œí’ˆ ì „ì²´#ê°€ê²©', 'ì œí’ˆ ì „ì²´#ë‹¤ì–‘ì„±', 'ì œí’ˆ ì „ì²´#ë””ìì¸', 'ì œí’ˆ ì „ì²´#ì¸ì§€ë„', 'ì œí’ˆ ì „ì²´#ì¼ë°˜', 'ì œí’ˆ ì „ì²´#í¸ì˜ì„±', 'ì œí’ˆ ì „ì²´#í’ˆì§ˆ',\n","    'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ê°€ê²©', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë‹¤ì–‘ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë””ìì¸', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ì¼ë°˜', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í¸ì˜ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í’ˆì§ˆ'\n","\n","]\n","more_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","\n","tf_id_to_name = ['True', 'False']\n","tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n","\n","polarity_id_to_name = ['positive', 'negative', 'neutral']\n","polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}"]},{"cell_type":"markdown","metadata":{},"source":["# Remove annotations without a target"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["total = pd.concat([train, dev])\n","\n","### Add prefix\n","trgs_prefix = 'Target '\n","# trgs_prefix = 'Target_1 Target_2 Target_3 '\n","trgs_prefix_len = len(trgs_prefix)\n","total['sentence_form'] = trgs_prefix + total['sentence_form']\n","# total"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Target \n","7\n","Target\n"]}],"source":["print(trgs_prefix)\n","print(trgs_prefix_len)\n","print(trgs_prefix[0:6])\n","# print(trgs_prefix[9:17])\n","# print(trgs_prefix[18:26])\n","trg = 'Target'\n","trg_rng = [0, 6]\n","# trgs = ['Target_1', 'Target_2', 'Target_3']\n","# trg_rngs = [[0, 8], [9, 17], [18, 26]]"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["### Remove\n","for idx, row in total.iterrows():\n","    # trg_idx = 0\n","    for idx in range(len(row.annotation)):\n","        if row.annotation[idx][1][0] == None:\n","            row.annotation[idx][1][0] = trg\n","            row.annotation[idx][1][1] = trg_rng[0]\n","            row.annotation[idx][1][2] = trg_rng[1]\n","\n","            # row.annotation[idx][1][0] = trgs[trg_idx]\n","            # row.annotation[idx][1][1] = trg_rngs[trg_idx][0]\n","            # row.annotation[idx][1][2] = trg_rngs[trg_idx][1]\n","            # trg_idx += 1\n","        else:\n","            row.annotation[idx][1][1] += trgs_prefix_len\n","            row.annotation[idx][1][2] += trgs_prefix_len\n","    row.annotation = [el for el in row.annotation if el != []]\n","    \n","# ### Remove\n","# for idx, row in total.iterrows():\n","#     for idx in range(len(row.annotation)):\n","#         if row.annotation[idx][1][0] == None:\n","#             row.annotation[idx] = []\n","#     row.annotation = [el for el in row.annotation if el != []]"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["### Check\n","for idx, row in total.iterrows():\n","    for annotation in row.annotation:\n","        if annotation[1][0] == None:\n","            print(row)        "]},{"cell_type":"markdown","metadata":{},"source":["# Drop rows without a single annotation"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["total['checker'] = total.annotation.apply(bool)\n","total = total[total.checker == True].copy()"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# count_tags(total, entity_property_pair)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["filter = [x for x in entity_property_pair if x not in ['ë³¸í’ˆ#ì¸ì§€ë„', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ê°€ê²©']]\n","total = remove_props(total, filter)\n","# count_tags(total, entity_property_pair)"]},{"cell_type":"markdown","metadata":{},"source":["# Split"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["total['stratified'] = total.annotation.apply(lambda x: x[0][0])\n","tagger_train, tagger_dev, _, _ = train_test_split(total, total['stratified'], test_size=0.2, random_state=42,  stratify=total['stratified'])\n","tagger_train.reset_index(inplace=True, drop=True)\n","tagger_dev.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# count_tags(tagger_train, entity_property_pair)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# count_tags(tagger_dev, entity_property_pair)"]},{"cell_type":"markdown","metadata":{},"source":["# Generate token classification pairs"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["train_split_inputs, train_split_labels = generate_token_classification_data(tagger_train)\n","dev_split_inputs, dev_split_labels = generate_token_classification_data(tagger_dev)\n","tagger_train['split_form'], tagger_train['split_label'] = train_split_inputs, train_split_labels\n","tagger_dev['split_form'], tagger_dev['split_label'] = dev_split_inputs, dev_split_labels"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize and Align"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["target_tagger_labels = ['Other', 'TRG_B', 'TRG_I']\n","labels = target_tagger_labels\n","label2id = {k: i for i, k in enumerate(labels)}\n","id2label = {i: k for i, k in enumerate(labels)}\n","num_labels = len(labels)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n","- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["({'Other': 0, 'TRG_B': 1, 'TRG_I': 2}, {0: 'Other', 1: 'TRG_B', 2: 'TRG_I'}, 3)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model_checkpoint = 'monologg/koelectra-base-v3-discriminator'\n","tokenizer = ElectraTokenizerFast.from_pretrained(model_checkpoint)\n","model = ElectraForTokenClassification.from_pretrained(\n","    model_checkpoint, label2id=label2id, id2label=id2label, num_labels=num_labels\n",")\n","model.config.label2id, model.config.id2label, model.num_labels"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["35000\n","\n","\n","\n","3060\n","35254\n"]},{"data":{"text/plain":["Embedding(35254, 768)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["### new\n","entity_property_pair = [\n","    'ë³¸í’ˆ#ê°€ê²©', 'ë³¸í’ˆ#ë‹¤ì–‘ì„±', 'ë³¸í’ˆ#ë””ìì¸', 'ë³¸í’ˆ#ì¸ì§€ë„', 'ë³¸í’ˆ#ì¼ë°˜', 'ë³¸í’ˆ#í¸ì˜ì„±', 'ë³¸í’ˆ#í’ˆì§ˆ',\n","    'ë¸Œëœë“œ#ê°€ê²©', 'ë¸Œëœë“œ#ë””ìì¸', 'ë¸Œëœë“œ#ì¸ì§€ë„', 'ë¸Œëœë“œ#ì¼ë°˜', 'ë¸Œëœë“œ#í’ˆì§ˆ',\n","    'ì œí’ˆ ì „ì²´#ê°€ê²©', 'ì œí’ˆ ì „ì²´#ë‹¤ì–‘ì„±', 'ì œí’ˆ ì „ì²´#ë””ìì¸', 'ì œí’ˆ ì „ì²´#ì¸ì§€ë„', 'ì œí’ˆ ì „ì²´#ì¼ë°˜', 'ì œí’ˆ ì „ì²´#í¸ì˜ì„±', 'ì œí’ˆ ì „ì²´#í’ˆì§ˆ',\n","    'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ê°€ê²©', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë‹¤ì–‘ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë””ìì¸', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ì¼ë°˜', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í¸ì˜ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í’ˆì§ˆ'\n","]\n","sentiments = ['positive', 'negative', 'neutral']\n","target = ['Target']\n","special_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","emojis = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))\n","ep_labels = pd.Series(entity_property_pair, name='sentence_form', copy=True)\n","\n","tokens2add = special_tokens + emojis\n","# tokens2add = special_tokens + emojis + entity_property_pair + sentiments + target\n","\n","print(len(tokenizer))\n","tokenizer_train_data = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame().drop_duplicates()\n","tokenizer_train_data = tokenizer_train_data.sentence_form.to_list()\n","new_tokenizer = tokenizer.train_new_from_iterator(tokenizer_train_data, vocab_size=1)\n","new_tokens = set(list(new_tokenizer.vocab.keys()) + tokens2add) - set(tokenizer.vocab.keys())\n","tokenizer.add_tokens(list(new_tokens))\n","print(len(new_tokenizer))\n","print(len(tokenizer))\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["input_tokens_list, labels = align_tokens_and_labels(tagger_train, tokenizer)\n","tagger_train['input_tokens_list'], tagger_train['labels'] = input_tokens_list, labels\n","input_tokens_list, labels = align_tokens_and_labels(tagger_dev, tokenizer)\n","tagger_dev['input_tokens_list'], tagger_dev['labels'] = input_tokens_list, labels"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Empty DataFrame\n","Columns: [id, sentence_form, annotation, checker, stratified, split_form, split_label, input_tokens_list, labels]\n","Index: []\n","Empty DataFrame\n","Columns: [id, sentence_form, annotation, checker, stratified, split_form, split_label, input_tokens_list, labels]\n","Index: []\n"]}],"source":["tagger_train['checker'] = tagger_train.input_tokens_list.apply(len) == tagger_train.labels.apply(len)\n","print(tagger_train[tagger_train.checker == False])\n","tagger_dev['checker'] = tagger_dev.input_tokens_list.apply(len) == tagger_dev.labels.apply(len)\n","print(tagger_dev[tagger_dev.checker == False])"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['ë³¸í’ˆ#í’ˆì§ˆ', ['Target', 0, 6], 'positive']]\n","Target ì´‰ì´‰í•œë° ëˆì ì´ì§€ë„ ì•Šì•„ì„œ ë” ì¢‹ì•„ìš”\n","['[CLS]', 'T', '##ar', '##ge', '##t', 'ì´‰ì´‰', '##í•œ', '##ë°', 'ëˆì ', '##ì´ì§€', '##ë„', 'ì•Š', '##ì•„', '##ì„œ', 'ë”', 'ì¢‹', '##ì•„', '##ìš”', '[SEP]']\n","[-100, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n","\n","[['ë³¸í’ˆ#í’ˆì§ˆ', ['í•˜ì™€ì´ ì½”ë‚˜ í•´ì–‘ ì‹¬ì¸µìˆ˜ì™€ ì™„ë„ ë¯¸ì—­ ì¶”ì¶œë¬¼í“¨ì½”ìŠ¤, ê·¸ ì™¸ 33ê°€ì§€ í•´ì–‘ì„±ë¶„', 7, 49], 'positive']]\n","Target í•˜ì™€ì´ ì½”ë‚˜ í•´ì–‘ ì‹¬ì¸µìˆ˜ì™€ ì™„ë„ ë¯¸ì—­ ì¶”ì¶œë¬¼í“¨ì½”ìŠ¤, ê·¸ ì™¸ 33ê°€ì§€ í•´ì–‘ì„±ë¶„ì´ ë“¤ì–´ê°€ í”¼ë¶€ ì§„ì • íš¨ê³¼, í”¼ë¶€ ì¥ë²½ ê°œì„  íš¨ê³¼, í”¼ë¶€ê²° ê°œì„ , ë¯¸ë°± ì£¼ë¦„ ê°œì„ ê¹Œì§€ ëŠë‚„ ìˆ˜ ìˆëŠ” ì™„ë²½í•œ í¬ë¦¼ì´ì—ìš”ğŸ‘ğŸ»\n","['[CLS]', 'T', '##ar', '##ge', '##t', 'í•˜ì™€ì´', 'ì½”', '##ë‚˜', 'í•´ì–‘', 'ì‹¬ì¸µ', '##ìˆ˜', '##ì™€', 'ì™„ë„', 'ë¯¸ì—­', 'ì¶”ì¶œë¬¼', '##í“¨', '##ì½”ìŠ¤', ',', 'ê·¸', 'ì™¸', '33', '##ê°€ì§€', 'í•´ì–‘', '##ì„±', '##ë¶„', 'ì´', 'ë“¤ì–´ê°€', 'í”¼ë¶€', 'ì§„ì •', 'íš¨ê³¼', ',', 'í”¼ë¶€', 'ì¥ë²½', 'ê°œì„ ', 'íš¨ê³¼', ',', 'í”¼ë¶€', '##ê²°', 'ê°œì„ ', ',', 'ë¯¸ë°±', 'ì£¼ë¦„', 'ê°œì„ ', '##ê¹Œ', '##ì§€', 'ëŠë‚„', 'ìˆ˜', 'ìˆ', '##ëŠ”', 'ì™„ë²½', '##í•œ', 'í¬ë¦¼', '##ì´', '##ì—', '##ìš”', 'ğŸ‘ğŸ»', '[SEP]']\n","[-100, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"]}],"source":["idx = random.randrange(len(tagger_train))\n","print(tagger_train.iloc[idx].annotation)\n","print(tagger_train.iloc[idx].sentence_form)\n","print(tagger_train.iloc[idx].input_tokens_list)\n","print(tagger_train.iloc[idx].labels)\n","print()\n","idx = random.randrange(len(tagger_dev))\n","print(tagger_dev.iloc[idx].annotation)\n","print(tagger_dev.iloc[idx].sentence_form)\n","print(tagger_dev.iloc[idx].input_tokens_list)\n","print(tagger_dev.iloc[idx].labels)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["[['íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë””ìì¸', ['íŒ¨ì§€í‚¤', 12, 15], 'positive'], ['ë³¸í’ˆ#í’ˆì§ˆ', ['Target', 0, 6], 'positive']]\n","toks = ['[CLS]', 'T', '##ar', '##ge', '##t', 'ê±°ê¸°', '##ë‹¤ê°€', 'íŒ¨', '##ì§€', '##í‚¤', 'ê¹Œì§€', 'ì˜ˆì˜', '##ê³ ', 'ê³„', '##ë©´', '##í™œ', '##ì„±', '##ì œ', '##ë„', 'ì—†', '##ê³ ', '.', '.', '[SEP]']\n","labs = [-100, 1, 2, 2, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["[['T', '##ar', '##ge', '##t'], ['íŒ¨', '##ì§€', '##í‚¤']]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["starts = list({k:v for k, v in enumerate(labs) if v == 1}.keys())\n","targets = []\n","for start in starts:\n","    target = [toks[start]]\n","    for tok, lab in zip(toks[start+1:], labs[start+1:]):\n","        if lab != 2:\n","            break\n","        else:\n","            target.append(tok)\n","    targets.append(target)\n","targets"]},{"cell_type":"markdown","metadata":{},"source":["# Save Tagger Data"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["./dataset/uncleaned_v10\n"]}],"source":["DATA_V = 'uncleaned_v9'\n","save_path = f'./dataset/{DATA_V}'\n","print(save_path)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!mkdir -p {save_path}\n","\n","train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","tagger_train.to_json(f'{save_path}/tagger_train.json')\n","tagger_dev.to_json(f'{save_path}/tagger_dev.json')\n","\n","# ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","# ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","\n","# p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","# p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Filter entity_property_pair and Drop rows accordingly"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FILTER_MODE = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if FILTER_MODE == True:\n","    filter = get_filter()\n","    train = remove_props(train, filter)\n","    dev = remove_props(dev, filter)\n","len(train), len(dev)"]},{"cell_type":"markdown","metadata":{"id":"hkFQP8GW4SXT"},"source":["# Preprocess"]},{"cell_type":"markdown","metadata":{},"source":["## Cleansing"]},{"cell_type":"markdown","metadata":{"id":"PII4v9rJLzaV"},"source":["### Before"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666249062532,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"oBhyEPcfmFdB","outputId":"362f9678-bda2-4ded-f372-23c0e9fdd36d"},"outputs":[],"source":["# for el in train.sample(n=5).sentence_form:\n","#     print(el)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5570,"status":"ok","timestamp":1666249070279,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"zxaftwE_uq9w"},"outputs":[],"source":["# train.sentence_form = train.sentence_form.apply(preprocess)\n","# dev.sentence_form = dev.sentence_form.apply(preprocess)\n","# test.sentence_form = test.sentence_form.apply(preprocess)\n","# total = pd.concat([train, dev])"]},{"cell_type":"markdown","metadata":{"id":"B28qDpz-L47B"},"source":["### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1666249157277,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"TmQV4tDyycbX"},"outputs":[],"source":["# case = total.sentence_form.str.contains('r[^A-Za-z0-9ê°€-í£\\s]+', case=False, flags=0, na=None, regex=True)\n","# for e in total[case].sentence_form:\n","#     print(e)"]},{"cell_type":"markdown","metadata":{"id":"ymHR_z-7L3LW"},"source":["### After"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666249070568,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"I94k-mG7s_q5","outputId":"9e82bf97-1530-4783-9fb3-80c82299a3c3"},"outputs":[],"source":["# for i, row in total[['id', 'sentence_form']].sample(n=5).iterrows():\n","#     print(row.id, '\\t', row.sentence_form)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# total['check'] = total.sentence_form.str.find('OO')\n","# for row in total[total.check > -1].sentence_form:\n","#     print(row)\n","#     break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# total"]},{"cell_type":"markdown","metadata":{"id":"25AnAYGke6BQ"},"source":["## Reformat"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666184223688,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"cRZR6lTRED9s","outputId":"08f43287-453a-4377-fe1b-e7d8b3fe6fd8"},"outputs":[],"source":["len(entity_property_pair)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["decorate_form, decorate_acd_pair, decorate_asc_pair, decorate_acd_pair_split, decorate_asc_pair_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9K9UG5aybovX"},"outputs":[],"source":["def reformat(df):\n","    ep =[]\n","    p = []\n","    for index, row in df.iterrows():\n","        utterance = row.sentence_form\n","        id = row.id\n","        \n","        form = utterance\n","        # form = decorate_form(utterance)\n","\n","        for pair in entity_property_pair:\n","            isPairInOpinion = False\n","            if pd.isna(utterance):\n","                break\n","            for annotation in row.annotation:\n","                entity_property = annotation[0]\n","                sentiment = annotation[2]\n","                if entity_property == pair:\n","                    \n","                    acd_pair = entity_property\n","                    # acd_pair = decorate_acd_pair(entity_property)\n","                    # acd_pair = decorate_acd_pair_split(entity_property)\n","                    \n","                    ep_append = [id, form, acd_pair, tf_name_to_id['True']]\n","                    ep.append(ep_append)\n","                    p.append([id, utterance, entity_property, sentiment])\n","                    isPairInOpinion = True\n","                    break\n","            if isPairInOpinion is False:\n","                \n","                acd_pair = pair\n","                # acd_pair = decorate_acd_pair(pair)\n","                # acd_pair = decorate_acd_pair_split(pair)\n","                \n","                ep_append = [id, form, acd_pair, tf_name_to_id['False']]\n","                ep.append(ep_append)\n","    return ep, p"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reformat_p_binary(df):\n","    p_binary = []\n","    for i, row in df.iterrows():\n","        row.id, row.form, row.pair, row.sentiment\n","        \n","        form = row.form\n","        # form = decorate_form(row.form)\n","        \n","        for sentiment in polarity_id_to_name:\n","            if sentiment == row.sentiment:\n","\n","                asc_pair = '#'.join([row.pair, row.sentiment])\n","                # asc_pair = decorate_asc_pair(row.pair, row.sentiment)\n","                # asc_pair = decorate_asc_pair_split(row.pair, row.sentiment)\n","\n","                p_binary_append = [row.id, form, asc_pair, tf_name_to_id['True']]\n","                p_binary.append(p_binary_append)\n","            else:\n","\n","                asc_pair = '#'.join([row.pair, sentiment])\n","                # asc_pair = decorate_asc_pair(row.pair, sentiment)\n","                # asc_pair = decorate_asc_pair_split(row.pair, sentiment)\n","\n","                p_binary_append = [row.id, form, asc_pair, tf_name_to_id['False']]\n","                p_binary.append(p_binary_append)\n","    return p_binary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(train), len(dev)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["entity_property_pair = [\n","    'ë³¸í’ˆ#ê°€ê²©', 'ë³¸í’ˆ#ë‹¤ì–‘ì„±', 'ë³¸í’ˆ#ë””ìì¸', 'ë³¸í’ˆ#ì¸ì§€ë„', 'ë³¸í’ˆ#ì¼ë°˜', 'ë³¸í’ˆ#í¸ì˜ì„±', 'ë³¸í’ˆ#í’ˆì§ˆ',\n","    'ë¸Œëœë“œ#ê°€ê²©', 'ë¸Œëœë“œ#ë””ìì¸', 'ë¸Œëœë“œ#ì¸ì§€ë„', 'ë¸Œëœë“œ#ì¼ë°˜', 'ë¸Œëœë“œ#í’ˆì§ˆ',\n","    'ì œí’ˆ ì „ì²´#ê°€ê²©', 'ì œí’ˆ ì „ì²´#ë‹¤ì–‘ì„±', 'ì œí’ˆ ì „ì²´#ë””ìì¸', 'ì œí’ˆ ì „ì²´#ì¸ì§€ë„', 'ì œí’ˆ ì „ì²´#ì¼ë°˜', 'ì œí’ˆ ì „ì²´#í¸ì˜ì„±', 'ì œí’ˆ ì „ì²´#í’ˆì§ˆ',\n","    'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ê°€ê²©', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë‹¤ì–‘ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë””ìì¸', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ì¼ë°˜', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í¸ì˜ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í’ˆì§ˆ'\n","\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unJ9pUXwYTTm"},"outputs":[],"source":["ep_train, p_train = reformat(train)\n","ep_dev, p_dev = reformat(dev)\n","\n","ep_train = pd.DataFrame(ep_train, columns=['id', 'form', 'pair', 'labels'])\n","ep_dev = pd.DataFrame(ep_dev, columns=['id', 'form', 'pair', 'labels'])\n","\n","p_train = pd.DataFrame(p_train, columns=['id', 'form', 'pair', 'sentiment'])\n","p_dev = pd.DataFrame(p_dev, columns=['id', 'form', 'pair', 'sentiment'])\n","\n","len(ep_train), len(ep_dev), len(p_train), len(p_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["p_binary_train = reformat_p_binary(p_train)\n","p_binary_train = pd.DataFrame(p_binary_train, columns=['id', 'form', 'pair', 'labels'])\n","\n","p_binary_dev = reformat_p_binary(p_dev)\n","p_binary_dev = pd.DataFrame(p_binary_dev, columns=['id', 'form', 'pair', 'labels'])\n","\n","len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ep_train.sort_values(['id', 'labels'], inplace=True)\n","# ep_dev.sort_values(['id', 'labels'], inplace=True)\n","# p_binary_train.sort_values(['id', 'labels'], inplace=True, ascending=[True, True])\n","# p_binary_dev.sort_values(['id', 'labels'], inplace=True, ascending=[True, True])"]},{"cell_type":"markdown","metadata":{"id":"rjaoyM97e9PQ"},"source":["### Counting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"GFroeNPQc-DZ","outputId":"bce5e0c2-0423-4db7-982a-23d5b68b8618"},"outputs":[],"source":["print('binary_multi: ', end=''), print(len(ep_train), len(ep_dev), len(p_train), len(p_dev))\n","print('binary_binary: ', end=''), print(len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev))\n","ep_train = ep_train.drop_duplicates()\n","ep_dev = ep_dev.drop_duplicates()\n","p_train = p_train.drop_duplicates()\n","p_dev = p_dev.drop_duplicates()\n","p_binary_train = p_binary_train.drop_duplicates()\n","p_binary_dev = p_binary_dev.drop_duplicates()\n","print('\\nafter drop_duplicates\\n')\n","print('binary_multi: ', end=''), print(len(ep_train), len(ep_dev), len(p_train), len(p_dev))\n","print('binary_binary: ', end=''), print(len(ep_train), len(ep_dev), len(p_binary_train), len(p_binary_dev))"]},{"cell_type":"markdown","metadata":{},"source":["### Validate Here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = ep_train\n","for idx, row in df.iterrows():\n","    print(row.id, '\\n',\n","          row.form, '\\n',\n","          row.pair, '\\n',\n","          row.labels,  '\\n',)\n","    if idx == 49:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["### Save"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_V = 'uncleaned_v6'\n","save_path = f'./dataset/{DATA_V}'\n","print(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir -p {save_path}\n","\n","train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","\n","p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Additional Length Test If Needed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ep_train, ep_dev, p_binary_train, p_binary_dev"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_checkpoint = 'snunlp/KR-ELECTRA-discriminator'\n","\n","train_path = f'./dataset/{DATA_V}/raw_train.csv'\n","dev_path = f'./dataset/{DATA_V}/raw_dev.csv'\n","test_path = f'./dataset/{DATA_V}/raw_test.csv'\n","train = pd.read_csv(train_path)\n","dev = pd.read_csv(dev_path)\n","test = pd.read_csv(test_path)\n","\n","### new\n","entity_property_pair = [\n","    'ë³¸í’ˆ#ê°€ê²©', 'ë³¸í’ˆ#ë‹¤ì–‘ì„±', 'ë³¸í’ˆ#ë””ìì¸', 'ë³¸í’ˆ#ì¸ì§€ë„', 'ë³¸í’ˆ#ì¼ë°˜', 'ë³¸í’ˆ#í¸ì˜ì„±', 'ë³¸í’ˆ#í’ˆì§ˆ',\n","    'ë¸Œëœë“œ#ê°€ê²©', 'ë¸Œëœë“œ#ë””ìì¸', 'ë¸Œëœë“œ#ì¸ì§€ë„', 'ë¸Œëœë“œ#ì¼ë°˜', 'ë¸Œëœë“œ#í’ˆì§ˆ',\n","    'ì œí’ˆ ì „ì²´#ê°€ê²©', 'ì œí’ˆ ì „ì²´#ë‹¤ì–‘ì„±', 'ì œí’ˆ ì „ì²´#ë””ìì¸', 'ì œí’ˆ ì „ì²´#ì¸ì§€ë„', 'ì œí’ˆ ì „ì²´#ì¼ë°˜', 'ì œí’ˆ ì „ì²´#í¸ì˜ì„±', 'ì œí’ˆ ì „ì²´#í’ˆì§ˆ',\n","    'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ê°€ê²©', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë‹¤ì–‘ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ë””ìì¸', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#ì¼ë°˜', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í¸ì˜ì„±', 'íŒ¨í‚¤ì§€/êµ¬ì„±í’ˆ#í’ˆì§ˆ'\n","]\n","special_tokens = ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","emojis = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))\n","ep_labels = pd.Series(entity_property_pair, name='sentence_form', copy=True)\n","\n","tokens2add = special_tokens + emojis\n","\n","tokenizer = ElectraTokenizerFast.from_pretrained(model_checkpoint)\n","print(len(tokenizer))\n","tokenizer_train_data = pd.concat([train.sentence_form, dev.sentence_form, test.sentence_form], ignore_index=True, verify_integrity=True).to_frame().drop_duplicates()\n","tokenizer_train_data = tokenizer_train_data.sentence_form.to_list()\n","new_tokenizer = tokenizer.train_new_from_iterator(tokenizer_train_data, vocab_size=1)\n","new_tokens = set(list(new_tokenizer.vocab.keys()) + tokens2add) - set(tokenizer.vocab.keys())\n","tokenizer.add_tokens(list(new_tokens))\n","print(len(new_tokenizer))\n","print(len(tokenizer))\n","# model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ep_train, ep_dev, p_binary_train, p_binary_dev\n","len_counter = []\n","for df in [ep_train, ep_dev, p_binary_train, p_binary_dev]:\n","    for idx, row in df.iterrows():\n","        len_counter.append(len(tokenizer(row[\"form\"], row[\"pair\"], truncation=True).input_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max(len_counter)"]},{"cell_type":"markdown","metadata":{},"source":["### done here."]},{"cell_type":"markdown","metadata":{},"source":["## Save Files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save_path = './dataset/cleaned_v1'\n","\n","# train.to_csv(f'{save_path}/raw_train.csv', index=False)\n","# dev.to_csv(f'{save_path}/raw_dev.csv', index=False)\n","# test.to_csv(f'{save_path}/raw_test.csv', index=False)\n","\n","# ep_train.to_csv(f'{save_path}/ce_train.csv', index=False)\n","# ep_dev.to_csv(f'{save_path}/ce_dev.csv', index=False)\n","# p_train.to_csv(f'{save_path}/pc_train.csv', index=False)\n","# p_dev.to_csv(f'{save_path}/pc_dev.csv', index=False)\n","# p_binary_train.to_csv(f'{save_path}/pc_binary_train.csv', index=False)\n","# p_binary_dev.to_csv(f'{save_path}/pc_binary_dev.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"jAr5_Zc6HHQV"},"source":["# ASC Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJxucvEoipu2"},"outputs":[],"source":["model_checkpoint = '/content/drive/MyDrive/aspect_based_sentiment_analysis/base_model/klue_roberta_base/v2/klue_roberta_base_mlm/checkpoint-19860'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOg2ROQQi2OQ"},"outputs":[],"source":["sTokens = tokenizer.all_special_tokens\n","\n","def delTokens(sent):\n","    sent = sent.split(' ')\n","    temp = []\n","    for e in sent:\n","        if e not in sTokens:\n","            temp.append(e)\n","    return ' '.join(temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abyMklzeI81o"},"outputs":[],"source":["positive, negative, neutral = p_train[p_train.sentiment == 'positive'], p_train[p_train.sentiment == 'negative'], p_train[p_train.sentiment == 'neutral']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666184227323,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Vl33p6WhHmOj","outputId":"ff54ba6a-ccd2-4fb2-f716-03a4469cc43d"},"outputs":[],"source":["len(positive), len(negative), len(neutral)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666184227323,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"Tt1lAdOUHDyq","outputId":"8c17fd2d-5a8b-4f7a-ac7b-8a69604a8281"},"outputs":[],"source":["(58 * 3) * 4 * 3, (95 * 3) * 4 * 2 # bt ri rr"]},{"cell_type":"markdown","metadata":{"id":"nI6TIyYLINET"},"source":["Back Translation / Random Insertion / Random Replacement / Random Swap / Random Deletion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeypyouVr6Vk"},"outputs":[],"source":["def backTrans(text):\n","    aug1 = ts.papago(text, sleep_seconds=5, from_language='ko', to_language='en')\n","    aug1 = ts.papago(aug1, sleep_seconds=5, from_language='en', to_language='ko')\n","\n","    aug2 = ts.papago(text, sleep_seconds=5, from_language='ko', to_language='ja')\n","    aug2 = ts.papago(aug2, sleep_seconds=5, from_language='ja', to_language='ko')\n","\n","    return [aug1, aug2]\n","\n","def randomInsert(num, sample, device):\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path=model_checkpoint, action=\"insert\", model_type='bert', top_k=5, aug_p=0.3, aug_min=1, aug_max=1, device=device)\n","\n","    aug_result = aug.augment(sample, n=num, num_thread=12)\n","    aug_result = list(map(delTokens, aug_result))\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomReplace(num, sample, device):\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path=model_checkpoint, action=\"insert\", model_type='bert', top_k=5, aug_p=0.3, aug_min=1, aug_max=1, device=device)\n","\n","    aug_result = aug.augment(sample, n=num, num_thread=12)\n","    aug_result = list(map(delTokens, aug_result))\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomSwap(num, sample):\n","    aug = naw.RandomWordAug(action='swap', aug_min=1, aug_max=1, aug_p=0.3)    \n","    aug_result = aug.augment(sample, n=num, num_thread=2)\n","    aug_result = list(set(aug_result))\n","    return aug_result\n","\n","def randomSplit(num, sample):\n","    aug = naw.SplitAug(aug_min=1, aug_max=1, aug_p=0.3, min_char=3)\n","    aug_result = aug.augment(sample, n=num, num_thread=2)\n","    aug_result = list(set(aug_result))\n","    return aug_result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666184227324,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"S61gxehDqAsf","outputId":"a5a79918-c784-41e5-a0f2-7f0006086908"},"outputs":[],"source":["(58 * 3) * 5 * 4, (95 * 3) * 4 * 3 # bt ri rr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QVPawzK3Uqt"},"outputs":[],"source":["def backtransRoutine(data2augment, output_path):\n","    print('back translation started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = backTrans(row[1])\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(len(f'back translation finished.\\ncurrent count: {len(data2augment)}'))\n","\n","    data_aug = pd.DataFrame(data2augment, columns=['id', 'sentence_form', 'entity_property', 'sentiment'])\n","    data_aug.to_csv(f'{output_path}', index=False)\n","\n","    return data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ab6cjfGYqpJG"},"outputs":[],"source":["import os\n","\n","def edaRoutine(data2augment, ri, rr, output_path):\n","    print(f'current count: {len(data2augment)}')\n","    print('random insertion started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = randomInsert(ri, row[1], 'cuda')\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(f'random insertion finished.\\ncurrent count: {len(data2augment)}')\n","\n","    print('random replacement started.')\n","    temp = []\n","    for row in data2augment:\n","        augs = randomReplace(rr, row[1], 'cuda')\n","        for aug in augs:\n","            if aug != '' and aug != row[1]:\n","                new = [row[0], aug, row[2], row[3]]\n","                if new not in data2augment:\n","                    temp.append(new)\n","    data2augment.extend(temp)\n","    print(f'random replacement finished.\\ncurrent count: {len(data2augment)}')\n","\n","    print('random swap and split started.')\n","    while len(data2augment) < len(positive):\n","        temp = []\n","        k = random.randrange(len(negative))\n","        id, text, entity, sentiment = data2augment[k]\n","\n","        selector = random.randint(0,1)\n","        if selector == 0:\n","            augs = randomSwap(1, text)\n","            for aug in augs:\n","                if aug != '' and aug != text:\n","                    new = [id, aug, entity, sentiment]\n","                    if new not in data2augment:\n","                        temp.append(new)\n","            data2augment.extend(temp)\n","        else:\n","            augs = randomSplit(1, text)\n","            for aug in augs:\n","                if aug != '' and aug != text:\n","                    new = [id, aug, entity, sentiment]\n","                    if new not in data2augment:\n","                        temp.append(new)\n","            data2augment.extend(temp)\n","        if len(data2augment)%25 == 0:\n","            print(f'random swap and split in progress.\\ncurrent count: {len(data2augment)}')\n","\n","    print(f'whole augmentation routine finished.\\ntotal count: {len(data2augment)}')\n","\n","    data_aug = pd.DataFrame(data2augment, columns=['id', 'sentence_form', 'entity_property', 'sentiment'])\n","    data_aug.to_csv(f'{output_path}', index=False)\n","\n","    return data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmx-J4bu1JuU"},"outputs":[],"source":["### negative\n","# # back translation\n","\n","# data2augment = negative.values.tolist()\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'negative_bt.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# negative_bt = backtransRoutine(data2augment, output_path)\n","negative_bt = pd.read_csv(output_path)\n","negative_bt = negative_bt.values.tolist()\n","# RI / RR\n","\n","ri = 4 # times - 1\n","rr = 3 # times - 1\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'negative_aug.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# negative_aug = edaRoutine(negative_bt, ri, rr, output_path)\n","negative_aug = pd.read_csv(output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qu5fuqt5Q6AY"},"outputs":[],"source":["negative_aug\n","negative_aug = negative_aug.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GUBfkU5QIZD"},"outputs":[],"source":["# negative_aug.sample(n=15, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eP_iTJrASqcr"},"outputs":[],"source":["# negative_aug.sort_values('id').head(50).sentence_form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XROBqEb71MmR"},"outputs":[],"source":["### neutral\n","# back translation\n","\n","# data2augment = neutral.values.tolist()\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'neutral_bt.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# neutral_bt = backtransRoutine(data2augment, output_path)\n","neutral_bt = pd.read_csv(output_path)\n","neutral_bt = neutral_bt.values.tolist()\n","\n","# RI / RR\n","\n","ri = 3 # times - 1\n","rr = 2 # times - 1\n","\n","output_folder = '/content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11'\n","output_file = 'neutral_aug.csv'\n","output_path = os.path.join(output_folder, output_file)\n","\n","# neutral_aug = edaRoutine(neutral_bt, ri, rr, output_path)\n","neutral_aug = pd.read_csv(output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MS1g9SKERC1T"},"outputs":[],"source":["neutral_aug\n","neutral_aug = neutral_aug.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC38BqB7RFMr"},"outputs":[],"source":["# neutral_aug.sample(n=15, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v5pqYIlRheZ"},"outputs":[],"source":["# neutral_aug.sort_values('id').head(50).sentence_form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Birg9KWWHGY2"},"outputs":[],"source":["p_train_aug = pd.concat([positive, negative_aug, neutral_aug])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"010dmjkolD3N"},"outputs":[],"source":["def reformat_p_binary(df):\n","    p_binary = []\n","    for i, row in df.iterrows():\n","        row.id, row.sentence_form, row.entity_property, row.sentiment\n","        for sentiment in polarity_id_to_name:\n","            if sentiment == row.sentiment:\n","                p_binary.append([row.id, row.sentence_form, '#'.join([row.entity_property, row.sentiment]), tf_name_to_id['True']])\n","            else: \n","                p_binary.append([row.id, row.sentence_form, '#'.join([row.entity_property, sentiment]), tf_name_to_id['False']])\n","    return p_binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffHy0u-5kH9J"},"outputs":[],"source":["p_binary_train_aug = reformat_p_binary(p_train_aug)\n","p_binary_train_aug = pd.DataFrame(p_binary_train_aug, columns=['id', 'sentence_form', 'entity_property', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666184235131,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"TcjTW80FTkU5","outputId":"9487802d-6ab8-431c-e551-a01c0eee48ba"},"outputs":[],"source":["p_binary_train_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue-_eGNSkJpk"},"outputs":[],"source":["p_binary_dev = reformat_p_binary(p_dev)\n","p_binary_dev = pd.DataFrame(p_binary_dev, columns=['id', 'sentence_form', 'entity_property', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"F7JUwP81T7t7","outputId":"99179898-45db-4e4a-c226-8992dd4d7112"},"outputs":[],"source":["p_binary_dev"]},{"cell_type":"markdown","metadata":{"id":"rjaoyM97e9PQ"},"source":["# Counting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"GFroeNPQc-DZ","outputId":"bce5e0c2-0423-4db7-982a-23d5b68b8618"},"outputs":[],"source":["len(ep_train), len(ep_dev), len(p_train), len(p_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666184237228,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"-UNi4guSpiOH","outputId":"59306b7c-8d3a-412d-dd30-b4ab2536de09"},"outputs":[],"source":["len(ep_train), len(ep_dev), len(p_binary_train_aug), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666184237229,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"NB39FuJa_vNH","outputId":"0d143fe1-f1c8-4a3b-bcb4-fbc95fa05433"},"outputs":[],"source":["ep_train = ep_train.drop_duplicates()\n","p_binary_train_aug = p_binary_train_aug.drop_duplicates()\n","ep_dev = ep_dev.drop_duplicates()\n","p_binary_dev = p_binary_dev.drop_duplicates()\n","len(ep_train), len(ep_dev), len(p_binary_train_aug), len(p_binary_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9RmADlISEwu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PxiYbGKaFfpE"},"source":["# Export"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1102,"status":"ok","timestamp":1666184351766,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"7AJTuriR2Wsd","outputId":"db6fdeaa-7d72-4bfd-d9fb-13114631b1b1"},"outputs":[],"source":["%cd /content/drive/MyDrive/aspect_based_sentiment_analysis/data/v11\n","\n","# train.to_csv('raw_train.csv', index=False)\n","# dev.to_csv('raw_dev.csv', index=False)\n","# test.to_csv('raw_test.csv', index=False)\n","\n","ep_train.to_csv('ce_train.csv', index=False)\n","p_binary_train_aug.to_csv('pc_binary_train_aug.csv', index=False)\n","ep_dev.to_csv('ce_dev.csv', index=False)\n","p_binary_dev.to_csv('pc_binary_dev.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP_NZbG8fEDN"},"outputs":[],"source":["# emojis = pd.concat([ep_train.sentence_form, p_train.sentence_form, ep_dev.sentence_form, p_dev.sentence_form], ignore_index=True, verify_integrity=True).to_frame()\n","# emojis = list(set(demoji.findall(' '.join(emojis.sentence_form.to_list())).keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"executionInfo":{"elapsed":964,"status":"ok","timestamp":1666184397981,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"UyUS26wMZRCu","outputId":"7d5ce0bc-49ac-48dd-9373-21f42f766349"},"outputs":[],"source":["df = pd.read_csv('ce_train.csv')\n","df[df.id == 'nikluge-sa-2022-train-00065']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1666184401423,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"izy5bCjIDTus","outputId":"03dcb49e-d238-4cd4-db55-73771d78026c"},"outputs":[],"source":["df = pd.read_csv('ce_dev.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1666184405643,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"4oEfZBxBK6bG","outputId":"151fe3a5-269b-4968-ca15-a755fa237ae9"},"outputs":[],"source":["df = pd.read_csv('pc_binary_train_aug.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666184410093,"user":{"displayName":"Jeonghyeon Park","userId":"12513544746873038725"},"user_tz":-540},"id":"SEDxWfFhDWDs","outputId":"0726487f-297a-4de9-8738-d24ba8680fe1"},"outputs":[],"source":["df = pd.read_csv('pc_binary_dev.csv')\n","df"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 ('jeonghyeon')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"}}},"nbformat":4,"nbformat_minor":0}
